{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# 7 MUSCLES CLASSIFICATION + (INTER/INTRA)PERSONAL VALIDATION",
   "metadata": {
    "collapsed": false
   },
   "id": "9a6d23e0d0743caa"
  },
  {
   "cell_type": "code",
   "id": "37441250891c70ee",
   "metadata": {},
   "source": [
    "# Installation of BioSignalsNotebooks\n",
    "# %pip install biosignalsnotebooks\n",
    "# %pip install tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e2efabe2412d086",
   "metadata": {},
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import biosignalsnotebooks as bsnb\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "# Tensorflow Model\n",
    "import tensorflow as tf\n",
    "from functools import reduce  # This is fine if you're using the 'reduce' function in your script\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import InputLayer, Conv1D, LeakyReLU, MaxPooling1D, LSTM, GlobalAveragePooling1D, Dense, Dropout, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "# Weight and Biases\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "\n",
    "# Model Metrics\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Creating the sEMG Signal Dataframes from the CSV files",
   "metadata": {
    "collapsed": false
   },
   "id": "fed287a164bc3c08"
  },
  {
   "cell_type": "code",
   "id": "e4c2a1c3ac0dd0a3",
   "metadata": {},
   "source": [
    "# # Function to create a dataframe to store the data of every subject across all trials \n",
    "# def subject_df_creator(subject_id, muscles_of_interest):\n",
    "#     trials_dfs_list = []\n",
    "#     output_msg=[]\n",
    "# \n",
    "#     for trial_number in tqdm(range(1, 51), desc=f\"Concatenating trial Files for Subject AB{subject_id}\"):\n",
    "#         filename = f\"5362627/AB{subject_id}/AB{subject_id}/Raw/AB{subject_id}_Circuit_0{trial_number:02d}_raw.csv\"\n",
    "#         if not os.path.exists(filename):  # Check if the file exists\n",
    "#             output_msg.append(f\"0{trial_number:02d}\")\n",
    "#             continue\n",
    "# \n",
    "#         df_trial = pd.read_csv(filename)\n",
    "#         df_trial_combined = pd.DataFrame()\n",
    "# \n",
    "#         # Concatenate 'Right_' and 'Left_' values for each muscle of interest\n",
    "#         for i in range(len(muscles_of_interest)):\n",
    "#             df_trial_combined[muscles_of_interest[i]] = pd.concat([df_trial['Right_'+muscles_of_interest[i]], df_trial['Left_'+muscles_of_interest[i]]], ignore_index=True)\n",
    "# \n",
    "#         trials_dfs_list.append(df_trial_combined)\n",
    "# \n",
    "#     # Concatenate all DataFrames in the list along the rows axis\n",
    "#     merged_df = pd.concat(trials_dfs_list, ignore_index=True)\n",
    "#     if output_msg:\n",
    "#         print(f\"{len(output_msg)} Files do not exist:\", output_msg)\n",
    "#     return merged_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Defining important lists\n",
    "subjects = [\"156\", \"185\", \"186\", \"188\", \"189\", \"190\", \"191\", \"192\", \"193\", \"194\"]\n",
    "muscles = ['TA', 'MG', 'SOL', 'BF', 'ST', 'VL', 'RF']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3a82d6844b895f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "674c2f5bcc6b8187",
   "metadata": {},
   "source": [
    "# # Creating dataframes to save subject-specific data\n",
    "# df_subject_156 = subject_df_creator(\"156\", muscles)\n",
    "# df_subject_185 = subject_df_creator(\"185\", muscles)\n",
    "# df_subject_186 = subject_df_creator(\"186\", muscles)\n",
    "# df_subject_188 = subject_df_creator(\"188\", muscles)\n",
    "# df_subject_189 = subject_df_creator(\"189\", muscles)\n",
    "# df_subject_190 = subject_df_creator(\"190\", muscles)\n",
    "# df_subject_191 = subject_df_creator(\"191\", muscles)\n",
    "# df_subject_192 = subject_df_creator(\"192\", muscles)\n",
    "# df_subject_193 = subject_df_creator(\"193\", muscles)\n",
    "# df_subject_194 = subject_df_creator(\"194\", muscles)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# # Saving the Dataframes into pickle objects to save time\n",
    "# df_subject_156.to_pickle('pickled_dataframes/df_subject_156.pkl')\n",
    "# df_subject_185.to_pickle('pickled_dataframes/df_subject_185.pkl')\n",
    "# df_subject_186.to_pickle('pickled_dataframes/df_subject_186.pkl')\n",
    "# df_subject_188.to_pickle('pickled_dataframes/df_subject_188.pkl')\n",
    "# df_subject_189.to_pickle('pickled_dataframes/df_subject_189.pkl')\n",
    "# df_subject_190.to_pickle('pickled_dataframes/df_subject_190.pkl')\n",
    "# df_subject_191.to_pickle('pickled_dataframes/df_subject_191.pkl')\n",
    "# df_subject_192.to_pickle('pickled_dataframes/df_subject_192.pkl')\n",
    "# df_subject_193.to_pickle('pickled_dataframes/df_subject_193.pkl')\n",
    "# df_subject_194.to_pickle('pickled_dataframes/df_subject_194.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71aa6e6992196281",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Loading pickle files into DataFrames\n",
    "df_subject_156 = pd.read_pickle('pickled_dataframes/df_subject_156.pkl')\n",
    "df_subject_185 = pd.read_pickle('pickled_dataframes/df_subject_185.pkl')\n",
    "df_subject_186 = pd.read_pickle('pickled_dataframes/df_subject_186.pkl')\n",
    "df_subject_188 = pd.read_pickle('pickled_dataframes/df_subject_188.pkl')\n",
    "df_subject_189 = pd.read_pickle('pickled_dataframes/df_subject_189.pkl')\n",
    "df_subject_190 = pd.read_pickle('pickled_dataframes/df_subject_190.pkl')\n",
    "df_subject_191 = pd.read_pickle('pickled_dataframes/df_subject_191.pkl')\n",
    "df_subject_192 = pd.read_pickle('pickled_dataframes/df_subject_192.pkl')\n",
    "df_subject_193 = pd.read_pickle('pickled_dataframes/df_subject_193.pkl')\n",
    "df_subject_194 = pd.read_pickle('pickled_dataframes/df_subject_194.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9418bcf6748f92f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "477e4fb02daa7f93",
   "metadata": {},
   "source": [
    "# Merging all the subject dataframes into one\n",
    "list_of_all_subjects_dfs = [df_subject_156, df_subject_185, df_subject_186, df_subject_188, df_subject_189,\n",
    "                            df_subject_190, df_subject_191, df_subject_192, df_subject_193, df_subject_194]\n",
    "\n",
    "df_all_subjects = pd.concat(list_of_all_subjects_dfs, ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "885a9899ab761bf2",
   "metadata": {},
   "source": "df_all_subjects",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "44146ec683a0fc26",
   "metadata": {},
   "source": "## EMG Signal Visualisation "
  },
  {
   "cell_type": "code",
   "id": "2423d44728d53dbd",
   "metadata": {},
   "source": [
    "# # adding figures and traces\n",
    "# fig1 = go.Figure()\n",
    "# fig1.add_trace(go.Scatter(x=df_all_subjects.index/1000, y=df_all_subjects['TA'][60000:90000]))\n",
    "# fig1.update_layout( title=\"sEMG Signal: Sitting Vs Contraction Bursts Vs Rest\", xaxis_title=\"Time (s)\",\n",
    "#                     yaxis_title=\"sEMG Activity (V)\", margin=dict(l=50, r=50, b=50, t=50, pad=4),\n",
    "#                     autosize=False, width=800, height=301)\n",
    "# # plotting\n",
    "# fig1.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e77c58d1388c023",
   "metadata": {},
   "source": "## EMG Signal Analysis"
  },
  {
   "cell_type": "code",
   "id": "b5bcc96368fa6ee6",
   "metadata": {},
   "source": [
    "# Studying mean, sigma and variance of the 2 Muscles\n",
    "df_analysis = pd.DataFrame()\n",
    "df_analysis['Mean'] = df_all_subjects.mean()\n",
    "df_analysis['Std'] = df_all_subjects.std()\n",
    "df_analysis['Var'] = df_all_subjects.var()\n",
    "df_analysis"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Iterate through each subject's DataFrame to calculate and print statistics\n",
    "for idx, df in enumerate(list_of_all_subjects_dfs):\n",
    "    # Create a DataFrame for the current subject with all statistics\n",
    "    subject_stats_df = pd.DataFrame({'Mean': df.mean(), 'Std': df.std(),'Var': df.var()}).transpose()  # Transpose to have statistics as columns\n",
    "\n",
    "    # Printing the table for the current subject\n",
    "    print(f\"- EMG Analysis for Subject {subjects[idx]}:\")\n",
    "    print(tabulate(subject_stats_df.round(4), headers='keys', tablefmt='grid'))\n",
    "    print(\"\\n\")  "
   ],
   "id": "27d0d63f5a4e7bad",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Burst Detection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81ea87cc5fdbb0f1"
  },
  {
   "cell_type": "code",
   "id": "88b8ed3bff3a2166",
   "metadata": {},
   "source": [
    "# # Saving the detected bursts for every muscle \n",
    "# sr = 1000 # sample rate = 1000Hz\n",
    "# sl = 20 # smooth level (Size of sliding window used during the moving average process) #used to be 40\n",
    "# th = 10 # threshold (To cover activation)\n",
    "# \n",
    "# # Initializing lists\n",
    "# detected_bursts_TA = [] ; detected_bursts_MG = [] ; detected_bursts_SOL= []\n",
    "# detected_bursts_BF = [] ; detected_bursts_ST = [] ; detected_bursts_VL = []\n",
    "# detected_bursts_RF = []\n",
    "# \n",
    "# pbar = tqdm(total=len(muscles)*len(list_of_all_subjects_dfs), desc=\"All Subjects Burst Detection Progress\", unit= \"Muscle\")\n",
    "# for df_subject in list_of_all_subjects_dfs:\n",
    "#     ## TA\n",
    "#     detected_bursts_TA.append(bsnb.detect_emg_activations(emg_signal=df_subject['TA'], sample_rate=sr, smooth_level=sl,\n",
    "#                                                           threshold_level=th, time_units=True, device='CH0', plot_result= False))\n",
    "#     pbar.update(1)  # Update progress bar\n",
    "#     ## MG\n",
    "#     detected_bursts_MG.append(bsnb.detect_emg_activations(emg_signal=df_subject['MG'], sample_rate=sr, smooth_level=sl,\n",
    "#                                                           threshold_level=th, time_units=True, device='CH0', plot_result= False))\n",
    "#     pbar.update(1)\n",
    "#     ## SOL\n",
    "#     detected_bursts_SOL.append(bsnb.detect_emg_activations(emg_signal=df_subject['SOL'], sample_rate=sr, smooth_level=sl,\n",
    "#                                                            threshold_level=th, time_units=True, device='CH0', plot_result= False))\n",
    "#     pbar.update(1)\n",
    "#     ## BF\n",
    "#     detected_bursts_BF.append(bsnb.detect_emg_activations(emg_signal=df_subject['BF'], sample_rate=sr, smooth_level=sl,\n",
    "#                                                           threshold_level=th, time_units=True, device='CH0', plot_result= False))\n",
    "#     pbar.update(1)\n",
    "#     ## ST\n",
    "#     detected_bursts_ST.append(bsnb.detect_emg_activations(emg_signal=df_subject['ST'], sample_rate=sr, smooth_level=sl,\n",
    "#                                                           threshold_level=th, time_units=True, device='CH0', plot_result= False))\n",
    "#     pbar.update(1)\n",
    "#     ## VL\n",
    "#     detected_bursts_VL.append(bsnb.detect_emg_activations(emg_signal=df_subject['VL'], sample_rate=sr, smooth_level=sl,\n",
    "#                                                           threshold_level=th, time_units=True, device='CH0', plot_result= False))\n",
    "#     pbar.update(1)\n",
    "#     ## RF\n",
    "#     detected_bursts_RF.append(bsnb.detect_emg_activations(emg_signal=df_subject['RF'], sample_rate=sr, smooth_level=sl,\n",
    "#                                                           threshold_level=th, time_units=True, device='CH0', plot_result= False))\n",
    "#     pbar.update(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# # Creating the pickles to save the burst detection results (saves 29 minutes)\n",
    "# with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_TA.pkl', 'wb') as f: pickle.dump(detected_bursts_TA, f)\n",
    "# with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_MG.pkl', 'wb') as f: pickle.dump(detected_bursts_MG, f)\n",
    "# with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_SOL.pkl','wb') as f: pickle.dump(detected_bursts_SOL,f)\n",
    "# with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_BF.pkl', 'wb') as f: pickle.dump(detected_bursts_BF, f)\n",
    "# with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_ST.pkl', 'wb') as f: pickle.dump(detected_bursts_ST, f)\n",
    "# with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_VL.pkl', 'wb') as f: pickle.dump(detected_bursts_VL, f)\n",
    "# with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_RF.pkl', 'wb') as f: pickle.dump(detected_bursts_RF, f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32689628ce37e44c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Loading the pickles\n",
    "pbar = tqdm(total=len(muscles), desc=\"Burst Detection Loading Progress\", unit= \"Muscle\")\n",
    "with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_TA.pkl', 'rb') as f: detected_bursts_TA = pickle.load(f)\n",
    "pbar.update(1)\n",
    "with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_MG.pkl', 'rb') as f: detected_bursts_MG = pickle.load(f)\n",
    "pbar.update(1)\n",
    "with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_SOL.pkl','rb') as f: detected_bursts_SOL= pickle.load(f)\n",
    "pbar.update(1)\n",
    "with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_BF.pkl', 'rb') as f: detected_bursts_BF = pickle.load(f)\n",
    "pbar.update(1)\n",
    "with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_ST.pkl', 'rb') as f: detected_bursts_ST = pickle.load(f)\n",
    "pbar.update(1)\n",
    "with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_VL.pkl', 'rb') as f: detected_bursts_VL = pickle.load(f)\n",
    "pbar.update(1)\n",
    "with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_RF.pkl', 'rb') as f: detected_bursts_RF = pickle.load(f)\n",
    "pbar.update(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66629be7075b0eda",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# pd.DataFrame(detected_bursts_SOL[:][:10]).transpose()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6511bb972b75fcd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Detected Bursts Visualisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b633b4b9cdb961f4"
  },
  {
   "cell_type": "code",
   "source": [
    "# # Visualising the EMG Burst Detection for SOL\n",
    "# plot_duration = 40000 # time in milliseconds\n",
    "# bsnb.detect_emg_activations(emg_signal = df_all_subjects['SOL'][:plot_duration], sample_rate = sr, smooth_level=sl, threshold_level=th, time_units=True, device='CH0', plot_result= True)\n",
    "# print('')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "996120a536564717",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4f73c254a64ba3c",
   "metadata": {},
   "source": [
    "# # Visualizing First Activations -> SEE CAPTURED WINDOW WITH RESPECT OF IDENTIFIED ACTIVATION\n",
    "# duration = 8000\n",
    "# shift = 2000\n",
    "# number_bursts_to_plot = 1\n",
    "# \n",
    "# plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "# fig = plt.figure()\n",
    "# \n",
    "# plt.plot(df_all_subjects['TA'][:duration], color=\"cornflowerblue\")\n",
    "# for i in range(number_bursts_to_plot): # Plot first N bursts\n",
    "#     plt.axvline(detected_bursts_TA[0][i]*1000,color='red', label=\"Detected Burst Region\") # ONSET VERTICAL LINE\n",
    "#     plt.axvline(detected_bursts_TA[1][i]*1000,color='red') # OFFSET VERTICAL LINE\n",
    "#     plt.axvline(detected_bursts_TA[0][i]*1000+400,color='black', label=\"Onset Window (700ms)\") # ONSET VERTICAL LINE CORRECTED (START WINDOW)\n",
    "#     plt.axvline(detected_bursts_TA[0][i]*1000-100,color='black') # VERTICAL LINE (END WINDOW)\n",
    "# \n",
    "# plt.legend(loc=\"upper left\")\n",
    "# plt.xlim(shift,duration)\n",
    "# plt.grid()\n",
    "# plt.xlabel('Time (ms)', fontsize=10)\n",
    "# plt.ylabel('sEMG Intensity (V)', fontsize=10)\n",
    "# \n",
    "# # plt.savefig(\"Window.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f99ecb5e20d3e887",
   "metadata": {},
   "source": [
    "# # adding figures and traces\n",
    "# fig1 = go.Figure()\n",
    "# fig1.add_trace(go.Scatter(x = df_all_subjects.index/1000 , y=df_all_subjects['Left_TA'][:10000]))\n",
    "# \n",
    "# # formatting the plot\n",
    "# fig1.update_layout(autosize=True, title=\"sEMG Signal: Detected burst and corrected onset window\",\n",
    "#                    xaxis_title=\"Time (s)\", yaxis_title=\"sEMG Activity (V)\", margin=dict(l=50, r=50, b=50, t=50, pad=4))\n",
    "# \n",
    "# fig1.add_vrect(x0=detected_bursts_TA[0][0], x1=detected_bursts_left_TA[1][0], row=\"all\", col=1,\n",
    "#                annotation_text=\"Detected Burst\", annotation_position=\"top right\", fillcolor=\"gray\",\n",
    "#                opacity=0.25, line_width=0)\n",
    "# \n",
    "# fig1.add_vline(x=detected_bursts_left_TA[0][0]+0.4,line_width=1.5, line_dash=\"dot\", line_color=\"red\")\n",
    "# fig1.add_vline(x=detected_bursts_left_TA[0][0]-0.1,line_width=1.5, line_dash=\"dot\", line_color=\"red\",\n",
    "#                annotation_text=\"Onset Window\",annotation_position=\"bottom right\")\n",
    "# \n",
    "# # fig1.update_xaxes(range=[7.5, 20000/1000])\n",
    "# # fig1.update_yaxes(range=[-2, 2])\n",
    "# fig1.update_layout(autosize=False, width=800, height=301)\n",
    "# # plotting\n",
    "# fig1.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3fc9b4c4fbb786b",
   "metadata": {},
   "source": [
    "# # adding figures and traces\n",
    "# fig1 = go.Figure()\n",
    "# fig1.add_trace(go.Scatter(x= df_all_subjects.index/1000, y=df_all_subjects['TA']))\n",
    "# # formatting the plot\n",
    "# fig1.update_layout(autosize=True, title=\"sEMG Signal: Detection of Activation Bursts\",\n",
    "#                    xaxis_title=\"Time (s)\", yaxis_title=\"sEMG Activity (V)\",\n",
    "#                    margin=dict(l=50, r=50, b=50, t=50, pad=4))\n",
    "# \n",
    "# for i in range(len(detected_bursts_TA[0])):\n",
    "#     fig1.add_vrect(x0=detected_bursts_TA[0][i], x1=detected_bursts_TA[1][i], row=\"all\", col=1,\n",
    "#                    annotation_text=\"Detected Burst\", annotation_position=\"top right\",\n",
    "#                    fillcolor=\"black\", opacity=0.25, line_width=0)\n",
    "# \n",
    "# # fig1.update_xaxes(range=[30, 60])\n",
    "# fig1.update_layout(autosize=False, width=800, height=301)\n",
    "# # plotting\n",
    "# fig1.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Detected Bursts Analysis "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "269ba9449138c886"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Number of Detected Bursts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaf5b33d7626e0e8"
  },
  {
   "cell_type": "code",
   "source": [
    "# Defining Variables to store the total number of bursts for every muscle \n",
    "tot_bursts_TA = [] ; tot_bursts_MG = [] ; tot_bursts_SOL = []\n",
    "tot_bursts_BF = [] ; tot_bursts_ST = [] ; tot_bursts_VL = []\n",
    "tot_bursts_RF = [] ; burst_count_list_for_printing = []\n",
    "\n",
    "# Calculating the total number of bursts per muscle\n",
    "for subject_idx, subject in enumerate(subjects):\n",
    "    tot_bursts_TA.append(len(detected_bursts_TA[subject_idx][0]))\n",
    "    tot_bursts_MG.append(len(detected_bursts_MG[subject_idx][0]))\n",
    "    tot_bursts_SOL.append(len(detected_bursts_SOL[subject_idx][0]))\n",
    "    tot_bursts_BF.append(len(detected_bursts_BF[subject_idx][0]))\n",
    "    tot_bursts_ST.append(len(detected_bursts_ST[subject_idx][0]))\n",
    "    tot_bursts_VL.append(len(detected_bursts_VL[subject_idx][0]))\n",
    "    tot_bursts_RF.append(len(detected_bursts_RF[subject_idx][0]))\n",
    "\n",
    "    # Calculate the total bursts for each subject across all muscles\n",
    "    total_bursts_subject = (tot_bursts_TA[subject_idx] + tot_bursts_MG[subject_idx] +\n",
    "                            tot_bursts_SOL[subject_idx] + tot_bursts_BF[subject_idx] +\n",
    "                            tot_bursts_ST[subject_idx] + tot_bursts_VL[subject_idx] +\n",
    "                            tot_bursts_RF[subject_idx])\n",
    "\n",
    "    # Saving the results in a list for fancy printing\n",
    "    burst_count_list_for_printing.append([subject, tot_bursts_TA[subject_idx], tot_bursts_MG[subject_idx], tot_bursts_SOL[subject_idx],\n",
    "                                          tot_bursts_BF[subject_idx], tot_bursts_ST[subject_idx], tot_bursts_VL[subject_idx],\n",
    "                                          tot_bursts_RF[subject_idx], total_bursts_subject])\n",
    "\n",
    "# Adding the total row to the printing\n",
    "burst_count_list_for_printing.append(['* Total Sum', sum(tot_bursts_TA), sum(tot_bursts_MG), sum(tot_bursts_SOL), sum(tot_bursts_BF),\n",
    "                                      sum(tot_bursts_ST), sum(tot_bursts_VL), sum(tot_bursts_RF),\n",
    "                                      sum(tot_bursts_TA) + sum(tot_bursts_MG) + sum(tot_bursts_SOL) + sum(tot_bursts_BF) +\n",
    "                                      sum(tot_bursts_ST) + sum(tot_bursts_VL) + sum(tot_bursts_RF)])\n",
    "\n",
    "# Printing the table\n",
    "print(\"Number of Muscle Bursts Per Subject Per Muscle:\\n\")\n",
    "headers = [\"Subject\", \"TA Bursts\", \"MG Bursts\", \"SOL Bursts\", \"BF Bursts\", \"ST Bursts\", \"VL Bursts\", \"RF Bursts\", \"Total Bursts\"]\n",
    "print(tabulate(burst_count_list_for_printing, headers=headers, tablefmt='grid'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba4fbe0e78e31449",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Longest Detected Bursts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b26b447d9810906"
  },
  {
   "cell_type": "code",
   "source": [
    "# Lists to store longest burst per muscle for all subjects\n",
    "longest_bursts_TA = []; longest_bursts_MG = []; longest_bursts_SOL = []\n",
    "longest_bursts_BF = []; longest_bursts_ST = []; longest_bursts_VL = []\n",
    "longest_bursts_RF = []\n",
    "longest_burst_list_for_printing = []\n",
    "\n",
    "# Calculating the longest burst per muscle\n",
    "for subject_idx, subject in enumerate(subjects):\n",
    "    longest_TA = max(np.array(detected_bursts_TA[subject_idx][1])-np.array(detected_bursts_TA[subject_idx][0]))\n",
    "    longest_MG = max(np.array(detected_bursts_MG[subject_idx][1])-np.array(detected_bursts_MG[subject_idx][0]))\n",
    "    longest_SOL = max(np.array(detected_bursts_SOL[subject_idx][1])-np.array(detected_bursts_SOL[subject_idx][0]))\n",
    "    longest_BF = max(np.array(detected_bursts_BF[subject_idx][1])-np.array(detected_bursts_BF[subject_idx][0]))\n",
    "    longest_ST = max(np.array(detected_bursts_ST[subject_idx][1])-np.array(detected_bursts_ST[subject_idx][0]))\n",
    "    longest_VL = max(np.array(detected_bursts_VL[subject_idx][1])-np.array(detected_bursts_VL[subject_idx][0]))\n",
    "    longest_RF = max(np.array(detected_bursts_RF[subject_idx][1])-np.array(detected_bursts_RF[subject_idx][0]))\n",
    "\n",
    "    # Save longest burst per muscle for this subject\n",
    "    longest_bursts_TA.append(longest_TA)\n",
    "    longest_bursts_MG.append(longest_MG)\n",
    "    longest_bursts_SOL.append(longest_SOL)\n",
    "    longest_bursts_BF.append(longest_BF)\n",
    "    longest_bursts_ST.append(longest_ST)\n",
    "    longest_bursts_VL.append(longest_VL)\n",
    "    longest_bursts_RF.append(longest_RF)\n",
    "\n",
    "    # Calculate the longest burst across all muscles for the subject\n",
    "    longest_burst_subject = max(longest_TA, longest_MG, longest_SOL, longest_BF, longest_ST, longest_VL, longest_RF)\n",
    "\n",
    "    # Save the results in a list for printing\n",
    "    longest_burst_list_for_printing.append([subject, longest_TA, longest_MG, longest_SOL, longest_BF, longest_ST, longest_VL, longest_RF, longest_burst_subject])\n",
    "\n",
    "# Adding the longest burst across all subjects per muscle and overall\n",
    "longest_burst_list_for_printing.append(['* Longest Burst', max(longest_bursts_TA), max(longest_bursts_MG),\n",
    "                                        max(longest_bursts_SOL), max(longest_bursts_BF),\n",
    "                                        max(longest_bursts_ST), max(longest_bursts_VL),\n",
    "                                        max(longest_bursts_RF), max([max(longest_bursts_TA), max(longest_bursts_MG), max(longest_bursts_SOL),\n",
    "                                                                     max(longest_bursts_BF), max(longest_bursts_ST), max(longest_bursts_VL), max(longest_bursts_RF)])])\n",
    "\n",
    "# Printing the table\n",
    "print(\"Longest Burst Per Subject Per Muscle:\\n\")\n",
    "headers = [\"Subject\", \"TA (s)\", \"MG (s)\", \"SOL (s)\", \"BF (s)\", \"ST (s)\", \"VL (s)\", \"RF (s)\", \"Per Subject (s)\"]\n",
    "print(tabulate(longest_burst_list_for_printing, headers=headers, tablefmt='grid'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2097a7200037a085",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Average Detected Bursts Lengths"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5900907130d06e68"
  },
  {
   "cell_type": "code",
   "id": "403d50b477df1e2f",
   "metadata": {},
   "source": [
    "# Lists to store average burst length per muscle for all subjects\n",
    "average_burst_lengths_TA = [] ; average_burst_lengths_MG = []\n",
    "average_burst_lengths_SOL = []; average_burst_lengths_BF = []\n",
    "average_burst_lengths_ST = [] ; average_burst_lengths_VL = []\n",
    "average_burst_lengths_RF = [] ; avg_burst_len_list_for_printing = []\n",
    "\n",
    "# Maximum allowed burst duration\n",
    "max_duration = 2\n",
    "\n",
    "# Calculating average burst length per muscle, excluding bursts longer than 2 seconds\n",
    "for subject_idx, subject in enumerate(subjects):\n",
    "    bursts_TA = np.array(detected_bursts_TA[subject_idx][1]) - np.array(detected_bursts_TA[subject_idx][0])\n",
    "    bursts_MG = np.array(detected_bursts_MG[subject_idx][1]) - np.array(detected_bursts_MG[subject_idx][0])\n",
    "    bursts_SOL = np.array(detected_bursts_SOL[subject_idx][1]) - np.array(detected_bursts_SOL[subject_idx][0])\n",
    "    bursts_BF = np.array(detected_bursts_BF[subject_idx][1]) - np.array(detected_bursts_BF[subject_idx][0])\n",
    "    bursts_ST = np.array(detected_bursts_ST[subject_idx][1]) - np.array(detected_bursts_ST[subject_idx][0])\n",
    "    bursts_VL = np.array(detected_bursts_VL[subject_idx][1]) - np.array(detected_bursts_VL[subject_idx][0])\n",
    "    bursts_RF = np.array(detected_bursts_RF[subject_idx][1]) - np.array(detected_bursts_RF[subject_idx][0])\n",
    "\n",
    "    # Filter bursts longer than 2 seconds\n",
    "    filtered_bursts_TA = bursts_TA[bursts_TA <= max_duration]\n",
    "    filtered_bursts_MG = bursts_MG[bursts_MG <= max_duration]\n",
    "    filtered_bursts_SOL = bursts_SOL[bursts_SOL <= max_duration]\n",
    "    filtered_bursts_BF = bursts_BF[bursts_BF <= max_duration]\n",
    "    filtered_bursts_ST = bursts_ST[bursts_ST <= max_duration]\n",
    "    filtered_bursts_VL = bursts_VL[bursts_VL <= max_duration]\n",
    "    filtered_bursts_RF = bursts_RF[bursts_RF <= max_duration]\n",
    "\n",
    "    # Calculate mean of filtered bursts\n",
    "    mean_TA = np.mean(filtered_bursts_TA)\n",
    "    mean_MG = np.mean(filtered_bursts_MG)\n",
    "    mean_SOL = np.mean(filtered_bursts_SOL)\n",
    "    mean_BF = np.mean(filtered_bursts_BF)\n",
    "    mean_ST = np.mean(filtered_bursts_ST)\n",
    "    mean_VL = np.mean(filtered_bursts_VL)\n",
    "    mean_RF = np.mean(filtered_bursts_RF)\n",
    "\n",
    "    average_burst_lengths_TA.append(mean_TA)\n",
    "    average_burst_lengths_MG.append(mean_MG)\n",
    "    average_burst_lengths_SOL.append(mean_SOL)\n",
    "    average_burst_lengths_BF.append(mean_BF)\n",
    "    average_burst_lengths_ST.append(mean_ST)\n",
    "    average_burst_lengths_VL.append(mean_VL)\n",
    "    average_burst_lengths_RF.append(mean_RF)\n",
    "\n",
    "    # Calculate the average for the subject across all muscles\n",
    "    subject_average = np.mean([mean_TA, mean_MG, mean_SOL, mean_BF, mean_ST, mean_VL, mean_RF])\n",
    "\n",
    "    # Saving the results in a list for fancy printing\n",
    "    avg_burst_len_list_for_printing.append([subject,\n",
    "                                            round(mean_TA, 3),\n",
    "                                            round(mean_MG, 3),\n",
    "                                            round(mean_SOL, 3),\n",
    "                                            round(mean_BF, 3),\n",
    "                                            round(mean_ST, 3),\n",
    "                                            round(mean_VL, 3),\n",
    "                                            round(mean_RF, 3),\n",
    "                                            round(subject_average, 3)])  # Include average per subject\n",
    "\n",
    "# Adding the total row to the printing\n",
    "avg_burst_len_list_for_printing.append(['* Avg. Len', round(np.mean(average_burst_lengths_TA), 3), round(np.mean(average_burst_lengths_MG), 3),\n",
    "                                        round(np.mean(average_burst_lengths_SOL), 3), round(np.mean(average_burst_lengths_BF), 3),\n",
    "                                        round(np.mean(average_burst_lengths_ST), 3), round(np.mean(average_burst_lengths_VL), 3),\n",
    "                                        round(np.mean(average_burst_lengths_RF), 3)])\n",
    "# Calculate the overall average length and append to the list for printing\n",
    "avg_burst_len_list_for_printing.append(['* Overall Avg. Len', round(np.mean(average_burst_lengths_TA + average_burst_lengths_MG + average_burst_lengths_SOL + average_burst_lengths_BF + average_burst_lengths_ST + average_burst_lengths_VL + average_burst_lengths_RF), 3)])\n",
    "\n",
    "# Printing the table\n",
    "print(f\"Average Burst Length in Seconds Per Subject Per Muscle (Excluding Muscle Bursts Longer than {max_duration} Seconds):\\n\")\n",
    "headers = [\"Subject\", \"TA Avg.Len\", \"MG Avg.Len\", \"SOL Avg.Len\", \"BF Avg.Len\", \"ST Avg.Len\", \"VL Avg.Len\", \"RF Avg.Len\", \"Avg. Per Subject\"]\n",
    "print(tabulate(avg_burst_len_list_for_printing, headers=headers, tablefmt='grid'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Histogram: Detected Bursts Lengths"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdff3aa7d77dc609"
  },
  {
   "cell_type": "code",
   "source": [
    "# # Adjust subplot indexing\n",
    "# f, a = plt.subplots(4, 2)\n",
    "# f.set_size_inches(10, 20)\n",
    "# a = a.ravel()\n",
    "# bin_edges = np.arange(start=0, stop=2, step=0.05)  # Example for bins of width 0.05\n",
    "# colors = ['red', 'blue', 'green', 'orange', 'purple', 'black', 'pink', 'cyan', 'lime', 'yellow']\n",
    "# \n",
    "# for subject_idx, subject in enumerate(subjects):\n",
    "# \tsubject = \"subject_\" + subject\n",
    "# \ta[0].hist(np.array(detected_bursts_TA[subject_idx][1])-np.array(detected_bursts_TA[subject_idx][0]), bins=bin_edges, alpha=0.5, label=subject, color=colors[subject_idx])\n",
    "# \ta[1].hist(np.array(detected_bursts_MG[subject_idx][1])-np.array(detected_bursts_MG[subject_idx][0]), bins=bin_edges, alpha=0.5, label=subject, color=colors[subject_idx])\n",
    "# \ta[2].hist(np.array(detected_bursts_SOL[subject_idx][1])-np.array(detected_bursts_SOL[subject_idx][0]),bins=bin_edges, alpha=0.5, label=subject, color=colors[subject_idx])\n",
    "# \ta[3].hist(np.array(detected_bursts_BF[subject_idx][1])-np.array(detected_bursts_BF[subject_idx][0]), bins=bin_edges, alpha=0.5, label=subject, color=colors[subject_idx])\n",
    "# \ta[4].hist(np.array(detected_bursts_ST[subject_idx][1])-np.array(detected_bursts_ST[subject_idx][0]), bins=bin_edges, alpha=0.5, label=subject, color=colors[subject_idx])\n",
    "# \ta[5].hist(np.array(detected_bursts_VL[subject_idx][1])-np.array(detected_bursts_VL[subject_idx][0]), bins=bin_edges, alpha=0.5, label=subject, color=colors[subject_idx])\n",
    "# \ta[6].hist(np.array(detected_bursts_RF[subject_idx][1])-np.array(detected_bursts_RF[subject_idx][0]), bins=bin_edges, alpha=0.5, label=subject, color=colors[subject_idx])\n",
    "# \n",
    "# # Set legends, titles, and labels for each subplot\n",
    "# for muscle_idx, muscle in enumerate(muscles):\n",
    "# \ta[muscle_idx].legend(loc='upper right')\n",
    "# \ta[muscle_idx].set_title('Histogram Burst Duration: ' + muscle)\n",
    "# \ta[muscle_idx].set_xlabel(\"Burst Duration (seconds)\")\n",
    "# \ta[muscle_idx].set_ylabel(\"Occurrences\")\n",
    "# \ta[muscle_idx].set_xlim([0, 2])  # Remove to see how bad the burst detection is \n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bde553b753a37fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extracting Bursts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "969a614a9718648"
  },
  {
   "cell_type": "markdown",
   "source": "### Method: Using a Window size of 700ms including an onset of 100ms",
   "metadata": {
    "collapsed": false
   },
   "id": "7b4bb78f5c6e1c60"
  },
  {
   "cell_type": "code",
   "source": [
    "# def extract_burst_windows(muscle_emg_signal, onset_list, window_size, left_shift_value, muscle_name):\n",
    "# \tsampling_rate = 1000\n",
    "# \tall_subjects_burst_samples = []\n",
    "# \tfor subject_index in tqdm(range(len(onset_list)), desc=f\"Extracting Bursts for {muscle_name}\"):\n",
    "# \t\tcurrent_subject_bursts = []\n",
    "# \t\tii = -1\n",
    "# \t\tfor onset in onset_list[subject_index][0]:\n",
    "# \t\t\tii += 1\n",
    "# \t\t\tonset_ms = int(onset * sampling_rate) - left_shift_value\n",
    "# \t\t\tcurrent_sample_window = []\n",
    "# \t\t\tif onset != onset_list[subject_index][0][-1]:\n",
    "# \t\t\t\tif (onset_ms + window_size) < (onset_list[subject_index][0][ii + 1] * 1000):\n",
    "# \t\t\t\t\tfor time_step in range(window_size):\n",
    "# \t\t\t\t\t\tcurrent_sample_window.append(muscle_emg_signal[onset_ms + time_step])\n",
    "# \t\t\telse:\n",
    "# \t\t\t\tfor time_step in range(window_size):\n",
    "# \t\t\t\t\tcurrent_sample_window.append(muscle_emg_signal[onset_ms + time_step])\n",
    "# \n",
    "# \t\t\tif current_sample_window:\n",
    "# \t\t\t\tcurrent_sample_window -= np.mean(current_sample_window)\n",
    "# \t\t\t\tcurrent_subject_bursts.append(current_sample_window)\n",
    "# \t\tall_subjects_burst_samples.append(current_subject_bursts)\n",
    "# \treturn all_subjects_burst_samples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d5af44622972382",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "window = 1000 # in ms (total window size)\n",
    "left_shift = 100 # in ms (left shift from detected onset). See detected onset on the vertical red lines in the plots above"
   ],
   "id": "860dafbbb98d697",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# fixed_bursts_TA = extract_burst_windows(df_all_subjects['TA'], detected_bursts_TA, window, left_shift, 'TA')\n",
    "# fixed_bursts_MG = extract_burst_windows(df_all_subjects['MG'], detected_bursts_MG, window, left_shift, 'MG')\n",
    "# fixed_bursts_SOL= extract_burst_windows(df_all_subjects['SOL'],detected_bursts_SOL,window, left_shift, 'SOL')\n",
    "# fixed_bursts_BF = extract_burst_windows(df_all_subjects['BF'], detected_bursts_BF, window, left_shift, 'BF')\n",
    "# fixed_bursts_ST = extract_burst_windows(df_all_subjects['ST'], detected_bursts_ST, window, left_shift, 'ST')\n",
    "# fixed_bursts_VL = extract_burst_windows(df_all_subjects['VL'], detected_bursts_VL, window, left_shift, 'VL')\n",
    "# fixed_bursts_RF = extract_burst_windows(df_all_subjects['RF'], detected_bursts_RF, window, left_shift, 'RF')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a917103daca3829e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# # Defining Variables to store the total number of bursts for every muscle \n",
    "# tot_bursts_TA = []; tot_bursts_MG = []; tot_bursts_SOL = []; tot_bursts_BF = []\n",
    "# tot_bursts_ST = []; tot_bursts_VL = []; tot_bursts_RF = []\n",
    "# burst_count_list_for_printing = []\n",
    "# \n",
    "# # Calculating the total number of bursts per muscle\n",
    "# for subject_idx, subject in enumerate(subjects):\n",
    "#     tot_bursts_TA.append(len(fixed_bursts_TA[subject_idx]))\n",
    "#     tot_bursts_MG.append(len(fixed_bursts_MG[subject_idx]))\n",
    "#     tot_bursts_SOL.append(len(fixed_bursts_SOL[subject_idx]))\n",
    "#     tot_bursts_BF.append(len(fixed_bursts_BF[subject_idx]))\n",
    "#     tot_bursts_ST.append(len(fixed_bursts_ST[subject_idx]))\n",
    "#     tot_bursts_VL.append(len(fixed_bursts_VL[subject_idx]))\n",
    "#     tot_bursts_RF.append(len(fixed_bursts_RF[subject_idx]))\n",
    "# \n",
    "#     # Calculate the total bursts for each subject across all muscles\n",
    "#     total_bursts_subject = (tot_bursts_TA[subject_idx] + tot_bursts_MG[subject_idx] +\n",
    "#                             tot_bursts_SOL[subject_idx] + tot_bursts_BF[subject_idx] +\n",
    "#                             tot_bursts_ST[subject_idx] + tot_bursts_VL[subject_idx] +\n",
    "#                             tot_bursts_RF[subject_idx])\n",
    "# \n",
    "#     # Saving the results in a list for fancy printing\n",
    "#     burst_count_list_for_printing.append([subject, tot_bursts_TA[subject_idx], tot_bursts_MG[subject_idx], tot_bursts_SOL[subject_idx],\n",
    "#                                           tot_bursts_BF[subject_idx], tot_bursts_ST[subject_idx], tot_bursts_VL[subject_idx],\n",
    "#                                           tot_bursts_RF[subject_idx], total_bursts_subject])\n",
    "# \n",
    "# # Adding the total row to the printing\n",
    "# total_sum = [sum(tot_bursts_TA), sum(tot_bursts_MG), sum(tot_bursts_SOL), sum(tot_bursts_BF), sum(tot_bursts_ST), sum(tot_bursts_VL), sum(tot_bursts_RF)]\n",
    "# total_sum.append(sum(total_sum))  # Add total of totals\n",
    "# burst_count_list_for_printing.append(['* Total Sum'] + total_sum)\n",
    "# \n",
    "# # Printing the table\n",
    "# print(\"Number of Muscle Bursts Per Subject Per Muscle:\\n\")\n",
    "# headers = [\"Subject\", \"TA Bursts\", \"MG Bursts\", \"SOL Bursts\", \"BF Bursts\", \"ST Bursts\", \"VL Bursts\", \"RF Bursts\", \"Total Per Subject\"]\n",
    "# print(tabulate(burst_count_list_for_printing, headers=headers, tablefmt=\"grid\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93b757e534d75473",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TFRecords: Storing Training and Validation Datasets in Tensorflow Records\n",
    "\n",
    "Reference: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/tfrecord.ipynb#scrollTo=_e3g9ExathXP"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2833a9f67e2ae0f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write TFRecords"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a420f2551effee29"
  },
  {
   "cell_type": "code",
   "source": [
    "# def _float_feature(value):\n",
    "# \t\"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "# \treturn tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "# \n",
    "# def _int64_feature(value):\n",
    "# \t\"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "# \treturn tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "# \n",
    "# def extract_burst_windows_tfrecord(emg_series_complete, onset_lists, window_size, left_shift_value):\n",
    "# \t# Ensure unique file names for parallel processing or repeated calls\n",
    "# \tfile_name = f'tfrecords/all_dataset_{window}ms.tfrecord'\n",
    "# \twith tf.io.TFRecordWriter(file_name) as tfrecord:\n",
    "# \t\tfor muscle_index in tqdm(range(len(emg_series_complete)), desc=\"Extracting dataset to TFRecords (Fixed Window)\"):\n",
    "# \t\t\tfor subject_index in range(len(onset_lists[muscle_index])):\n",
    "# \t\t\t\tburst_count = 0\n",
    "# \t\t\t\tii = -1\n",
    "# \t\t\t\tfor onset in onset_lists[muscle_index][subject_index][0]:\n",
    "# \t\t\t\t\tii += 1\n",
    "# \t\t\t\t\tonset_ms = int(onset * 1000) - left_shift_value\n",
    "# \t\t\t\t\tcurrent_sample_window = []\n",
    "# \t\t\t\t\tif onset != onset_lists[muscle_index][subject_index][0][-1]:\n",
    "# \t\t\t\t\t\tif (onset_ms + window_size) < (onset_lists[muscle_index][subject_index][0][ii + 1] * 1000):\n",
    "# \t\t\t\t\t\t\tburst_count = burst_count + 1\n",
    "# \t\t\t\t\t\t\tfor time_step in range(window_size):\n",
    "# \t\t\t\t\t\t\t\tcurrent_sample_window.append(emg_series_complete[muscle_index][onset_ms + time_step])\n",
    "# \t\t\t\t\telse:\n",
    "# \t\t\t\t\t\tburst_count = burst_count + 1\n",
    "# \t\t\t\t\t\tfor time_step in range(window_size):\n",
    "# \t\t\t\t\t\t\tcurrent_sample_window.append(emg_series_complete[muscle_index][onset_ms + time_step])\n",
    "# \n",
    "# \t\t\t\t\tif current_sample_window:\n",
    "# \t\t\t\t\t\tcurrent_sample_window -= np.mean(current_sample_window)\n",
    "# \t\t\t\t\t\t# Convert your sample and label to appropriate tf.train.Feature formats\n",
    "# \t\t\t\t\t\tfeatures = {\n",
    "# \t\t\t\t\t\t\t'label': _int64_feature(muscle_index),\n",
    "# \t\t\t\t\t\t\t'feature': tf.train.Feature(float_list=tf.train.FloatList(value=current_sample_window)),\n",
    "# \t\t\t\t\t\t\t'subject': _int64_feature(subject_index + 1)\n",
    "# \t\t\t\t\t\t}\n",
    "# \t\t\t\t\t\texample = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "# \t\t\t\t\t\ttfrecord.write(example.SerializeToString())\n",
    "# \treturn file_name"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1ce5791af57c5dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# # .....\n",
    "# emg_series_tot = [df_all_subjects['TA'], df_all_subjects['MG'], df_all_subjects['SOL'],\n",
    "#                   df_all_subjects['BF'], df_all_subjects['ST'], df_all_subjects['VL'],\n",
    "#                   df_all_subjects['RF']]\n",
    "# \n",
    "# detected_bursts_tot = [detected_bursts_TA, detected_bursts_MG, detected_bursts_SOL,\n",
    "#                        detected_bursts_BF, detected_bursts_ST, detected_bursts_VL,\n",
    "#                        detected_bursts_RF]\n",
    "# # Extracting bursts\n",
    "# extract_burst_windows_tfrecord(emg_series_tot, detected_bursts_tot,  window, left_shift)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28d034070fed2670",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read TFRecords"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f069e88275cf16b6"
  },
  {
   "cell_type": "code",
   "source": [
    "# def map_fn(serialized_example):\n",
    "# \tfeatures = {\n",
    "# \t\t'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "# \t\t'feature': tf.io.FixedLenFeature([window], tf.float32),\n",
    "# \t\t'subject': tf.io.FixedLenFeature([], tf.int64)\n",
    "# \t}\n",
    "# \texample = tf.io.parse_single_example(serialized_example, features)\n",
    "# \treturn example['label'], example['feature'], example['subject']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adbbf7e6af15aea1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Iterate over the whole dataset to count records/samples (https://www.rustyrobotics.com/posts/tensorflow/tfdataset-record-count/)\n",
    "# Reference: https://www.rustyrobotics.com/posts/tensorflow/tfdataset-record-count/\n",
    "def countRecords(ds:tf.data.Dataset):\n",
    "\tcount = 0\n",
    "\tif tf.executing_eagerly():\n",
    "\t\t# TF v2 or v1 in eager mode\n",
    "\t\tfor _ in ds:\n",
    "\t\t\tcount = count+1\n",
    "\telse:\n",
    "\t\t# TF v1 in non-eager mode\n",
    "\t\titerator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
    "\t\tnext_batch = iterator.get_next()\n",
    "\t\twith tf.compat.v1.Session() as sess:\n",
    "\t\t\ttry:\n",
    "\t\t\t\twhile True:\n",
    "\t\t\t\t\tsess.run(next_batch)\n",
    "\t\t\t\t\tcount = count+1\n",
    "\t\t\texcept tf.errors.OutOfRangeError:\n",
    "\t\t\t\tpass\n",
    "\treturn count"
   ],
   "id": "e85f2446e9e3a817",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# dataset = tf.data.TFRecordDataset(f'tfrecords/all_dataset_{window}ms.tfrecord')\n",
    "# dataset = dataset.map(map_fn)\n",
    "# \n",
    "# for label, feature, subject in dataset.take(10):\n",
    "# \tprint(f'label={label}, Number of features={len(feature)}  subject={subject}')"
   ],
   "id": "d3e3a2e9dbcac170",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# def separate_dataset_per_subject_train_val(dataset_to_separate, sbj, training_percentage):\n",
    "# \t# Filtering whole dataset TFRECORDS by subjects:\n",
    "# \tdataset_subject = dataset_to_separate.filter(lambda label, feature, subject: subject == sbj)\n",
    "# \t# Count Total Samples for each Subject Dataset\n",
    "# \tdataset_subject_samples = countRecords(dataset_subject)\n",
    "# \t# Shuffling bursts per subject\n",
    "# \tdataset_subject_shuffled = dataset_subject.shuffle(dataset_subject_samples)\n",
    "# \t# Separating Subject Training and Evaluation Datasets:\n",
    "# \tdataset_subject1_train = dataset_subject_shuffled.take(int(dataset_subject_samples * training_percentage))\n",
    "# \tdataset_subject1_val = dataset_subject_shuffled.skip(int(dataset_subject_samples * training_percentage)).take(dataset_subject_samples - int(dataset_subject_samples * training_percentage))\n",
    "# \treturn dataset_subject_shuffled, dataset_subject1_train, dataset_subject1_val"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a60fc34550fd0511",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# # Training/Validation Split\n",
    "# train_percentage = 0.8\n",
    "# dataset_subject_1, dataset_subject_1_train, dataset_subject_1_val = separate_dataset_per_subject_train_val(dataset, 1, train_percentage)\n",
    "# dataset_subject_2, dataset_subject_2_train, dataset_subject_2_val = separate_dataset_per_subject_train_val(dataset, 2, train_percentage)\n",
    "# dataset_subject_3, dataset_subject_3_train, dataset_subject_3_val = separate_dataset_per_subject_train_val(dataset, 3, train_percentage)\n",
    "# dataset_subject_4, dataset_subject_4_train, dataset_subject_4_val = separate_dataset_per_subject_train_val(dataset, 4, train_percentage)\n",
    "# dataset_subject_5, dataset_subject_5_train, dataset_subject_5_val = separate_dataset_per_subject_train_val(dataset, 5, train_percentage)\n",
    "# dataset_subject_6, dataset_subject_6_train, dataset_subject_6_val = separate_dataset_per_subject_train_val(dataset, 6, train_percentage)\n",
    "# dataset_subject_7, dataset_subject_7_train, dataset_subject_7_val = separate_dataset_per_subject_train_val(dataset, 7, train_percentage)\n",
    "# dataset_subject_8, dataset_subject_8_train, dataset_subject_8_val = separate_dataset_per_subject_train_val(dataset, 8, train_percentage)\n",
    "# dataset_subject_9, dataset_subject_9_train, dataset_subject_9_val = separate_dataset_per_subject_train_val(dataset, 9, train_percentage)\n",
    "# dataset_subject_10,dataset_subject_10_train,dataset_subject_10_val= separate_dataset_per_subject_train_val(dataset, 10,train_percentage)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0375b2c1ecb8fe4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# for label, feature, subject in dataset_subject_10.take(10):\n",
    "# \tprint(f'label={label}, Number of features={len(feature)}  subject={subject}')"
   ],
   "id": "c45dd86d9da711e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# # ???????\n",
    "# all_subject_datasets_train = [dataset_subject_1_train, dataset_subject_2_train, dataset_subject_3_train,\n",
    "#                               dataset_subject_4_train, dataset_subject_5_train, dataset_subject_6_train,\n",
    "#                               dataset_subject_7_train, dataset_subject_8_train, dataset_subject_9_train,\n",
    "#                               dataset_subject_10_train]\n",
    "# \n",
    "# all_subject_datasets_val =   [dataset_subject_1_val, dataset_subject_2_val, dataset_subject_3_val,\n",
    "#                               dataset_subject_4_val, dataset_subject_5_val, dataset_subject_6_val,\n",
    "#                               dataset_subject_7_val, dataset_subject_8_val, dataset_subject_9_val,\n",
    "#                               dataset_subject_10_val]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94a7cd59e9b8e05c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# # ????????????\n",
    "# def augment_datasets(collection_datasets, tf_record_name):\n",
    "#     with tf.io.TFRecordWriter(tf_record_name) as tfrecord:\n",
    "#         # Check if the input is not a list (assuming a single dataset tuple)\n",
    "#         if not isinstance(collection_datasets, list):\n",
    "#             collection_datasets = [collection_datasets]  # Wrap it in a list if it's a single dataset\n",
    "# \n",
    "#         for dataset in collection_datasets:\n",
    "#             for label, feature, subject in dataset:\n",
    "#                 features = {\n",
    "#                     'label': tf.train.Feature(int64_list=tf.train.Int64List(value=np.asarray([label]))),\n",
    "#                     'feature': tf.train.Feature(float_list=tf.train.FloatList(value=np.asarray(feature))),\n",
    "#                     'subject': tf.train.Feature(int64_list=tf.train.Int64List(value=np.asarray([subject])))\n",
    "#                 }\n",
    "#                 example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "#                 tfrecord.write(example.SerializeToString())\n",
    "# \n",
    "# def map_fn_final(serialized_example):\n",
    "# \tfeatures = {\n",
    "# \t\t'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "# \t\t'feature': tf.io.FixedLenFeature([window], tf.float32),\n",
    "# \t\t'subject': tf.io.FixedLenFeature([], tf.int64)\n",
    "# \t}\n",
    "# \texample = tf.io.parse_single_example(serialized_example, features)\n",
    "# \treturn example['label'], example['feature'], example['subject']\n",
    "# \n",
    "# \n",
    "# def mix_shuffle_and_save_datasets(tf_record_read, tf_record_write):\n",
    "# \tdataset = tf.data.TFRecordDataset(tf_record_read)\n",
    "# \tdataset = dataset.map(map_fn_final)\n",
    "# \tdataset_samples = countRecords(dataset)\n",
    "# \tdataset_shuffled = dataset.shuffle(dataset_samples)\n",
    "# \n",
    "# \twith tf.io.TFRecordWriter(tf_record_write) as tfrecord:\n",
    "# \t\tfor label, feature, subject in dataset_shuffled:\n",
    "# \t\t\tfeatures = {\n",
    "# \t\t\t\t'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label.numpy()])),\n",
    "# \t\t\t\t'feature': tf.train.Feature(float_list=tf.train.FloatList(value=feature.numpy())),\n",
    "# \t\t\t\t'subject': tf.train.Feature(int64_list=tf.train.Int64List(value=[subject.numpy()]))\n",
    "# \t\t\t}\n",
    "# \t\t\texample = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "# \t\t\ttfrecord.write(example.SerializeToString())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe00b17fb70892b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Augment datasets \n",
    "# pbar = tqdm(total=len(list_of_all_subjects_dfs)+2, desc=\"Datasets Augmentation Progress\", unit= \"Dataset\")\n",
    "# augment_datasets(all_subject_datasets_train, f'tfrecords/augmented_train_{window}ms.tfrecord')\n",
    "# pbar.update(1)\n",
    "# augment_datasets(all_subject_datasets_val, f'tfrecords/augmented_val_{window}ms.tfrecord')\n",
    "# pbar.update(1)\n",
    "# augment_datasets(dataset_subject_1, f'tfrecords/augmented_dataset_subject_1_{window}ms')\n",
    "# pbar.update(1)\n",
    "# augment_datasets(dataset_subject_2, f'tfrecords/augmented_dataset_subject_2_{window}ms')\n",
    "# pbar.update(1)\n",
    "# augment_datasets(dataset_subject_3, f'tfrecords/augmented_dataset_subject_3_{window}ms')\n",
    "# pbar.update(1)\n",
    "# augment_datasets(dataset_subject_4, f'tfrecords/augmented_dataset_subject_4_{window}ms')\n",
    "# pbar.update(1)\n",
    "# augment_datasets(dataset_subject_5, f'tfrecords/augmented_dataset_subject_5_{window}ms')\n",
    "# pbar.update(1)\n",
    "# augment_datasets(dataset_subject_6, f'tfrecords/augmented_dataset_subject_6_{window}ms')\n",
    "# pbar.update(1)\n",
    "# augment_datasets(dataset_subject_7, f'tfrecords/augmented_dataset_subject_7_{window}ms')\n",
    "# pbar.update(1)\n",
    "# augment_datasets(dataset_subject_8, f'tfrecords/augmented_dataset_subject_8_{window}ms')\n",
    "# pbar.update(1)\n",
    "# augment_datasets(dataset_subject_9, f'tfrecords/augmented_dataset_subject_9_{window}ms')\n",
    "# pbar.update(1)\n",
    "# augment_datasets(dataset_subject_10,f'tfrecords/augmented_dataset_subject_10_{window}ms')\n",
    "# pbar.update(1)"
   ],
   "id": "d0d42cb584de28e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# #Mix and shuffle then save datasets \n",
    "# pbar = tqdm(total=len(list_of_all_subjects_dfs)+2, desc=\"Datasets \\\"Mix and Shuffle\\\" Progress\", unit= \"Dataset\")\n",
    "# mix_shuffle_and_save_datasets(f'tfrecords/augmented_train_{window}ms.tfrecord', f'tfrecords/all_mixed_train_{window}ms.tfrecord')\n",
    "# pbar.update(1)\n",
    "# mix_shuffle_and_save_datasets(f'tfrecords/augmented_val_{window}ms.tfrecord', f'tfrecords/all_mixed_val_{window}ms.tfrecord')\n",
    "# pbar.update(1)\n",
    "# mix_shuffle_and_save_datasets(f'tfrecords/augmented_dataset_subject_1_{window}ms', f'tfrecords/mixed_shuffled_subject_1_{window}ms')\n",
    "# pbar.update(1)\n",
    "# mix_shuffle_and_save_datasets(f'tfrecords/augmented_dataset_subject_2_{window}ms', f'tfrecords/mixed_shuffled_subject_2_{window}ms')\n",
    "# pbar.update(1)\n",
    "# mix_shuffle_and_save_datasets(f'tfrecords/augmented_dataset_subject_3_{window}ms', f'tfrecords/mixed_shuffled_subject_3_{window}ms')\n",
    "# pbar.update(1)\n",
    "# mix_shuffle_and_save_datasets(f'tfrecords/augmented_dataset_subject_4_{window}ms', f'tfrecords/mixed_shuffled_subject_4_{window}ms')\n",
    "# pbar.update(1)\n",
    "# mix_shuffle_and_save_datasets(f'tfrecords/augmented_dataset_subject_5_{window}ms', f'tfrecords/mixed_shuffled_subject_5_{window}ms')\n",
    "# pbar.update(1)\n",
    "# mix_shuffle_and_save_datasets(f'tfrecords/augmented_dataset_subject_6_{window}ms', f'tfrecords/mixed_shuffled_subject_6_{window}ms')\n",
    "# pbar.update(1)\n",
    "# mix_shuffle_and_save_datasets(f'tfrecords/augmented_dataset_subject_7_{window}ms', f'tfrecords/mixed_shuffled_subject_7_{window}ms')\n",
    "# pbar.update(1)\n",
    "# mix_shuffle_and_save_datasets(f'tfrecords/augmented_dataset_subject_8_{window}ms', f'tfrecords/mixed_shuffled_subject_8_{window}ms')\n",
    "# pbar.update(1)\n",
    "# mix_shuffle_and_save_datasets(f'tfrecords/augmented_dataset_subject_9_{window}ms', f'tfrecords/mixed_shuffled_subject_9_{window}ms')\n",
    "# pbar.update(1)\n",
    "# mix_shuffle_and_save_datasets(f'tfrecords/augmented_dataset_subject_10_{window}ms',f'tfrecords/mixed_shuffled_subject_10_{window}ms')\n",
    "# pbar.update(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4eaafc8f73cacd8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load TFRecords"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dac32b33347666fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 1024"
   ],
   "id": "9a869b4a78f96a29",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def read_tfrecord(serialized_example, export_subject=False):\n",
    "\ttfrecord_format = (\n",
    "\t\t{\n",
    "\t\t\t'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "\t\t\t'feature': tf.io.FixedLenFeature([window], tf.float32),\n",
    "\t\t\t'subject': tf.io.FixedLenFeature([], tf.int64)\n",
    "\t\t}\n",
    "\t)\n",
    "\texample = tf.io.parse_single_example(serialized_example, tfrecord_format)\n",
    "\tf = tf.reshape(example['feature'], [window,1])\n",
    "\tf.set_shape([window, 1])\n",
    "\t# One-hot encode the label to match the expected shape for categorical_crossentropy\n",
    "\tlabel = tf.one_hot(example['label'], depth=7) \n",
    "\tif export_subject:\n",
    "\t\treturn f, label, example['subject']\n",
    "\treturn f, label\n",
    "\n",
    "def get_dataset(tf_record_name, train_or_valid):\n",
    "\t# dataset = load_dataset(filename)\n",
    "\tdataset = tf.data.TFRecordDataset(tf_record_name)\n",
    "\tdataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n",
    "\tdataset_samples = countRecords(dataset)\n",
    "\tprint(f\" - Number of {train_or_valid} Samples: \", dataset_samples)\n",
    "\tdataset = dataset.shuffle(dataset_samples)\n",
    "\tdataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\tdataset = dataset.batch(BATCH_SIZE)\n",
    "\treturn dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48547a8332d332b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"## For Intrapersonal-CV:\")\n",
    "train_dataset = get_dataset(f'tfrecords/all_mixed_train_{window}ms.tfrecord', 'Train')\n",
    "valid_dataset = get_dataset(f'tfrecords/all_mixed_val_{window}ms.tfrecord', 'Validation')\n",
    "print(\"\\n ## For Interpersonal-LOSOCV:\")\n",
    "all_subjects_loo_data = [get_dataset(f'tfrecords/mixed_shuffled_subject_{i}_{window}ms', f\"Subject_{i}\") for i in range(1, 11)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e9f683dfc88431e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Verifying the shapes of: LOO, Training and Validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46dcef478b869dbf"
  },
  {
   "cell_type": "code",
   "source": [
    "print('# Training:')\n",
    "for feature, label in train_dataset:\n",
    "\tprint(f'\\t - label={label.shape}, feature={feature.shape}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d57b18567e25a41",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print('# Validation:')\n",
    "for feature, label in valid_dataset:\n",
    "\tprint(f'\\t - label={label.shape}, feature={feature.shape}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22217c55fb931b9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for subj_idx, subj in enumerate(all_subjects_loo_data):\t\n",
    "\tprint(f'# Subject_{subj_idx+1}:')\n",
    "\tfor feature, label in subj:\n",
    "\t\tprint(f'\\t - label={label.shape}, feature={feature.shape}')"
   ],
   "id": "9ecf6ac9c0ceacfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Plotting Muscle Bursts Samples",
   "metadata": {
    "collapsed": false
   },
   "id": "44b360a2c0a3a554"
  },
  {
   "cell_type": "code",
   "source": [
    "# # quick plot to see individual contraction bursts\n",
    "# def plot_independent_bursts(label, burst_list, color):\n",
    "#     n_cols = len(burst_list)  # number of subjects\n",
    "#     fig, axs = plt.subplots(1, n_cols, figsize=(15, 3), dpi=150)  # Adjusted figure size\n",
    "#     fig.subplots_adjust(top=0.85)  # Adjust top spacing\n",
    "#     fig.suptitle(f'Contraction Bursts: {label} ({window}ms Fixed Length)', y=0.95)  # Adjust title position\n",
    "# \n",
    "#     for j in range(n_cols):\n",
    "#         axs[j].plot(burst_list[j][0], color=color)  # Assuming you want the first burst\n",
    "#         axs[j].set_title(f'1st Burst of subject {j+1}', fontsize=10)\n",
    "#         axs[j].set_xlabel('time (ms)', fontsize=8)\n",
    "#         axs[j].set_ylabel('EMG', fontsize=8)\n",
    "#         axs[j].label_outer()  # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "# \n",
    "#     plt.tight_layout(pad=2.0, w_pad=0.5)  # Dynamically adjust layout\n",
    "#     plt.show()\n",
    "# \n",
    "# plot_independent_bursts(\"TA\", fixed_bursts_TA, \"cornflowerblue\")\n",
    "# plot_independent_bursts(\"MG\", fixed_bursts_MG, \"orange\")\n",
    "# plot_independent_bursts(\"SOL\",fixed_bursts_SOL,\"red\")\n",
    "# plot_independent_bursts(\"BF\", fixed_bursts_BF, \"green\")\n",
    "# plot_independent_bursts(\"ST\", fixed_bursts_ST, \"orange\")\n",
    "# plot_independent_bursts(\"VL\", fixed_bursts_VL, \"cyan\")\n",
    "# plot_independent_bursts(\"RF\", fixed_bursts_RF, \"brown\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dcb34cab3aec956",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Useful Functions for the Training",
   "id": "ce1c5d6e9e2100a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clear_and_create_directory(directory):\n",
    "\t\"\"\"Check if a directory exists and clear it, then recreate it.\"\"\"\n",
    "\tif os.path.exists(directory):\n",
    "\t\t# Remove the directory and all its contents\n",
    "\t\tshutil.rmtree(directory)\n",
    "\t\tprint(f\"Old \\\"{directory}\\\" directory of the previous model deleted!\")\n",
    "\t# Create the directory again\n",
    "\tos.makedirs(directory, exist_ok=True)"
   ],
   "id": "a9de83a7dee9a4fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plotting_loss_and_accuracy_over_epochs(history_name, title, is_to_show=False):\n",
    "\th = np.load(history_name+'.npy', allow_pickle=True).item()\n",
    "\n",
    "\tmin_val_categ_crossentropy = round(min(h['val_loss']), 4)\n",
    "\tbest_v_accu = round(max(h['val_accuracy']), 3) * 100\n",
    "\n",
    "\t# Find the epoch with the best validation accuracy\n",
    "\tbest_val_acc_epoch = np.argmax(h['val_accuracy'])\n",
    "\n",
    "\tfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\tfig.suptitle(title, fontsize=18, y=0.99)\n",
    "\tax1.set_title(f'Loss Function (min_categ_crossentropy={min_val_categ_crossentropy})')\n",
    "\tax2.set_title(f'Accuracy (best_val_acc={best_v_accu}%)')\n",
    "\tax1.set(xlabel='Epoch', ylabel='Loss (Categorical Crossentropy)')\n",
    "\tax2.set(xlabel='Epoch', ylabel='Accuracy')\n",
    "\tax1.plot(h['loss'], color=\"cornflowerblue\", linewidth=3)\n",
    "\tax1.plot(h['val_loss'], color=\"lightsteelblue\", linewidth=3)\n",
    "\tax1.legend(['Training Loss', 'Validation Loss'])\n",
    "\tax2.plot(h['accuracy'], color=\"gold\", linewidth=3)\n",
    "\tax2.plot(h['val_accuracy'], color=\"darkorange\", linewidth=3)\n",
    "\tax2.legend(['Training Accuracy', 'Validation Accuracy'])\n",
    "\t# Add a red 'X' mark at the epoch where the best validation accuracy occurs\n",
    "\tax2.scatter(best_val_acc_epoch, h['val_accuracy'][best_val_acc_epoch], color='red', marker='X', s=100)\n",
    "\tplt.savefig(f\"results_figures/{title}\")\n",
    "\t\n",
    "\tif is_to_show:\n",
    "\t\tplt.show()"
   ],
   "id": "b5a571272240bff3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def serializing_datasets(train_dataset_to_serialize, valid_dataset_to_serialize):\n",
    "    train_features_list = [] ; train_labels_list = []\n",
    "    valid_features_list = [] ; valid_labels_list = []\n",
    "\n",
    "    for feature_array, label_array in train_dataset_to_serialize:\n",
    "        for i in range(label_array.shape[0]):\n",
    "            train_features_list.append(feature_array[i])\n",
    "            train_labels_list.append(label_array[i])\n",
    "\t\t    \n",
    "    for feature_array, label_array in valid_dataset_to_serialize:\n",
    "        for i in range(label_array.shape[0]):\n",
    "            valid_features_list.append(feature_array[i])\n",
    "            valid_labels_list.append(label_array[i])\n",
    "\n",
    "    train_features = np.array(train_features_list); train_labels = np.array(train_labels_list)\n",
    "    valid_features = np.array(valid_features_list); valid_labels = np.array(valid_labels_list)\n",
    "    \n",
    "    return train_features, train_labels, valid_features, valid_labels"
   ],
   "id": "a7ed31e7850fd38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_confusion_matrix(x, y, color, title, is_to_show=False):\n",
    "\t# Find the filename of the best model saved during training\n",
    "\tbest_model_filename = max(glob.glob('model1/best_model_epoch_*_val_acc_*.h5'), key=os.path.getctime)\n",
    "\t# Load the best model\n",
    "\tbest_model = load_model(best_model_filename)\n",
    "\n",
    "\tpredictions_hot = best_model.predict(x)\n",
    "\tpredictions = np.argmax(predictions_hot, axis=1)\n",
    "\tnp.set_printoptions(suppress=True)\n",
    "\tprint(\"Some y Predicted\\t\", predictions[:30])\n",
    "\ty_list = np.argmax(y, axis=1)\n",
    "\tprint(\"Some y Label\\t\\t\", y_list[:30])\n",
    "\n",
    "\tcm = confusion_matrix(y_list, predictions)\n",
    "\tplt.figure(figsize=(15, 10))\n",
    "\tax = sn.heatmap(cm, annot=True, cmap=color, fmt='d')\n",
    "\tax.set_xlabel('Predicted Values')\n",
    "\tax.set_ylabel('Actual Values ')\n",
    "\tax.xaxis.set_ticklabels(['TA', 'MG', 'SOL', 'BF', 'ST', 'VL', 'RF'])\n",
    "\tax.yaxis.set_ticklabels(['TA', 'MG', 'SOL', 'BF', 'ST', 'VL', 'RF'])\n",
    "\n",
    "\tnp.set_printoptions(precision=3)\n",
    "\tprecision, recall, f1, _ = score(np.argmax(y, axis=1), np.argmax(predictions_hot, axis=1))\n",
    "\tf1_micro = f1_score(np.argmax(y, axis=1), np.argmax(predictions_hot, axis=1), average='micro')\n",
    "\tprint(f'precision: {precision}')\n",
    "\tprint(f'recall: {recall}')\n",
    "\tprint(f'fscore: {f1}')\n",
    "\tprint(f'fscore_micro: {f1_micro:.3f}')\n",
    "\n",
    "\ttitle = title + f'(F1score_micro = {f1_micro:.3})'\n",
    "\tax.set_title(title + '\\n\\n')\n",
    "\tax.xaxis.set_label_position('top')\n",
    "\tax.xaxis.set_ticks_position('top')\n",
    "\tplt.savefig(f\"results_figures/{title}.jpg\")\n",
    "\n",
    "\tif is_to_show:\n",
    "\t\tplt.show()\n",
    "\n",
    "\treturn precision, recall, f1, f1_micro"
   ],
   "id": "fa86f0b65ec8fd58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hybrid CNN-LSTM Model Implementation",
   "id": "106297cc060a5404"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check if GPU is available\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ],
   "id": "6f9ae9b8682270f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Definition",
   "id": "c8a7081f7b9e9281"
  },
  {
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "\n",
    "epochs = 200 # For an in depth performance testing\n",
    "# epochs = 5 # For quick  testing\n",
    "\n",
    "lr = 0.01 \n",
    "scheduler = ReduceLROnPlateau(factor=0.1, patience=15, min_lr=0.0001)  # Adjust patience as needed!!!!!!!\n",
    "\n",
    "    ## CNN Layer 1\n",
    "conv1D_1 = [32,5,1]  # number_filters,kernel_size and strides \n",
    "# conv1D_1 = [64,3,1]  # number_filters,kernel_size and strides \n",
    "dropout_1= 0.3      # Dropout %\n",
    "pool1D_1 = [2,2]     # pool_size and strides\n",
    "\n",
    "    ## CNN Layer 2\n",
    "conv1D_2 = [64,7,1]  # number_filters,kernel_size and strides\n",
    "# conv1D_2 = [96,5,1]  # number_filters,kernel_size and strides\n",
    "dropout_2= 0.3      # Dropout %\n",
    "pool1D_2 = [2,2]     # pool_size and strides\n",
    "\n",
    "#     ## CNN Layer 3\n",
    "conv1D_3 = [96,9,1] # number_filters,kernel_size and strides\n",
    "# conv1D_3 = [128,9,1] # number_filters,kernel_size and strides\n",
    "dropout_3= 0.3      # Dropout %\n",
    "pool1D_3 = [2,2]     # pool_size and strides\n",
    "\n",
    "    # Dense 1\n",
    "dense_1  = 50        # nodes ->50\n",
    "dense_dropout_1 = 0.3# Dropout %\n",
    "    # LSTM 1             \n",
    "lstm_1   = 30        # lstm blocks ->30\n",
    "lstm_dropout_1 = 0.3 # Dropout %\n",
    "    # Dense 3\n",
    "dense_3  = 15        # nodes ->15\n",
    "dense_dropout_3 = 0.3# Dropout %\n",
    "\t\n",
    "def model_creation():\t\n",
    "\t# Definition\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(InputLayer((window,1))) #InputLayer(BURST_WINDOW, N_CHANNELS)\n",
    "\t\n",
    "\t# CNN LAYER 1 (Conv1D + PReLU + MaxPooling + Dropout)\n",
    "\tmodel.add(Conv1D(filters=conv1D_1[0],kernel_size=conv1D_1[1], strides=conv1D_1[2],padding='same', name='cnn_layer_1')) # TODO TRY WITH HIGHER KERNEL SIZE (ODD NUMBER!)\n",
    "\tmodel.add(LeakyReLU(negative_slope=0.1))\n",
    "\tmodel.add(MaxPooling1D(pool_size=pool1D_1[0], strides=pool1D_1[1], padding='same'))\n",
    "\tmodel.add(Dropout(dropout_1))\n",
    "\t\n",
    "\t# CNN LAYER 2 (Conv1D + PReLU + MaxPooling + Dropout)\n",
    "\tmodel.add(Conv1D(filters=conv1D_2[0], kernel_size=conv1D_2[1], strides=conv1D_2[2], padding='same', name='cnn_layer_2'))\n",
    "\tmodel.add(LeakyReLU(negative_slope=0.1))\n",
    "\tmodel.add(MaxPooling1D(pool_size=pool1D_2[0], strides=pool1D_2[1], padding='same'))\n",
    "\tmodel.add(Dropout(dropout_2))\n",
    "\t\n",
    "\t# CNN LAYER 3 (Conv1D + LeakyReLU + MaxPooling + Dropout)\n",
    "\tmodel.add(Conv1D(filters=conv1D_3[0], kernel_size=conv1D_3[1], strides=conv1D_3[2], padding='same', name='cnn_layer_3'))\n",
    "\tmodel.add(LeakyReLU(negative_slope=0.1))\n",
    "\tmodel.add(MaxPooling1D(pool_size=pool1D_3[0], strides=pool1D_3[1], padding='same'))\n",
    "\tmodel.add(Dropout(dropout_3))\n",
    "\t\n",
    "\t# Global Average Pooling\n",
    "\tmodel.add(GlobalAveragePooling1D())\n",
    "\t\n",
    "\t# Dense 1:  To integrate the Dense Layer 1 effectively after GAP, we reshape the output to make it compatible with the dense layer expectations\n",
    "\tmodel.add(Reshape((1, -1))) \n",
    "\tmodel.add(Dense(dense_1))\n",
    "\tmodel.add(LeakyReLU(negative_slope=0.1))\n",
    "\tmodel.add(Dropout(dense_dropout_1))\n",
    "\t\n",
    "\t# LSTM LAYER 1 + Dropout\n",
    "\tmodel.add(LSTM(lstm_1, dropout=lstm_dropout_1))\n",
    "\t\n",
    "\t# Dense 3\n",
    "\tmodel.add(Dense(dense_3)) \n",
    "\tmodel.add(LeakyReLU(negative_slope=0.1))\n",
    "\tmodel.add(Dropout(dense_dropout_3))\n",
    "\t\n",
    "\t# Softmax\n",
    "\tmodel.add(Dense(7, 'softmax')) # Softmax\n",
    "\t\n",
    "\tprint(\"A fresh New Model Created!\")\n",
    "\t\n",
    "\treturn model\n",
    "\n",
    "model1 = model_creation()\n",
    "\n",
    "#Summary\n",
    "model1.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e1ce08a7df3b743",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Logging Wandb Parameters\n",
    "config = {\"number of muscles\": len(muscles),\n",
    "          \"number of subjects\": len(subjects), # used to be all_subject_dfs\n",
    "          \"LOSOCV\": \"On\",\n",
    "          \"batch_size\": BATCH_SIZE,\n",
    "          \"learning_rate\":lr,\n",
    "          \"epochs\": epochs,\n",
    "          \"scheduler\": \"ReduceLROnPlateau\",\n",
    "          \"optimizer\": \"Adam\",\n",
    "          \n",
    "          \"CNN_1\": conv1D_1,\n",
    "          \"CNN_1_Activation\": \"LeakyReLU\",\n",
    "          \"CNN_1_Pool\": pool1D_1,\n",
    "          \"CNN_1_dropout\": dropout_1,\n",
    "          \n",
    "          \"CNN_2\": conv1D_2,\n",
    "          \"CNN_2_Activation\": \"LeakyReLU\",\n",
    "          \"CNN_2_Pool\": pool1D_2,\n",
    "          \"CNN_2_dropout\": dropout_2,\n",
    "          \n",
    "          \"CNN_3\": conv1D_3,\n",
    "          \"CNN_3_Activation\": \"LeakyReLU\",\n",
    "          \"CNN_3_Pool\": pool1D_3,\n",
    "          \"CNN_3_dropout\": dropout_3,\n",
    "          \n",
    "          \"GAP\": \"On\",\n",
    "          \n",
    "          \"dense_1\": dense_1,\n",
    "          \"dense_1_dropout\": dense_dropout_1,\n",
    "          \n",
    "          \"lstm_1\": lstm_1,\n",
    "          \"lstm_1_dropout\": lstm_dropout_1,\n",
    "          \n",
    "          \"dense_3\": dense_3,\n",
    "          \"dense_3_dropout\": dense_dropout_3}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95ab9b2a04c48592",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Intrapersonal Performance Test: Cross-Validation",
   "id": "cbdcacdb559aac9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Deleting the old directories of the previously trained models\n",
    "clear_and_create_directory(\"model1\")\n",
    "clear_and_create_directory(\"histories\")\n",
    "clear_and_create_directory(\"results_figures\")"
   ],
   "id": "42fed72ce4d6e21d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialising Wandb logging\n",
    "wandb.init(project='Thesis', entity='firass-koli', config=config, name=\"Intrapersonal-CV\", group=\"Intra\")\n",
    "optimizer = Adam(learning_rate=wandb.config.learning_rate)"
   ],
   "id": "6df18b911ab67589",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model1.compile(loss=categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])  # TODO: Try DK (Learning rate)\n",
    "cp = ModelCheckpoint('model1/best_model_epoch_{epoch:02d}_val_acc_{val_accuracy:.4f}.keras', save_best_only=True, monitor='val_accuracy', mode='max')"
   ],
   "id": "689bc06bce13a33e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# with tf.device('/gpu:0'):\n",
    "# Training the model with the wandb callback\n",
    "history = model1.fit(train_dataset, validation_data=valid_dataset, epochs=wandb.config.epochs,\n",
    "                      callbacks=[cp, scheduler, WandbMetricsLogger(log_freq=5)])\n",
    "\n",
    "# Log the best validation accuracy and loss\n",
    "wandb.log({\"best_val_accuracy\": max(history.history['val_accuracy']), \"min_val_loss\": min(history.history['val_loss'])})"
   ],
   "id": "fb5104467a5a44eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ## In case of Keyboard Interrupt\n",
    "# wandb.finish()"
   ],
   "id": "61226f8aa9e7e14d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Retrieve minimum loss and best accuracy\n",
    "min_val_categ_crossentropy = round(min(history.history['val_loss']),3)\n",
    "best_val_accuracy = round(max(history.history['val_accuracy']),3)*100\n",
    "\n",
    "history_name = f'histories/history(val_acc={best_val_accuracy}%,val_categ_crossentropy={min_val_categ_crossentropy})'\n",
    "np.save(history_name + '.npy',history.history)\n",
    "# NOTE: The warnings you will see in the training are not relevant (it's due to the fact that the model is being saved so to be able to call it back in the future)\n",
    "print(f'History (loss and accuracy) for training and validation saved in:\\n-> {history_name}')"
   ],
   "id": "e07f3b733a5b3728",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Plotting Loss and Accuracy Metrics over Epochs\n",
    "plotting_loss_and_accuracy_over_epochs(history_name, f'Loss and Accuracy over Epochs (Intrapersonal)', True)\n",
    "\n",
    "## Serializing Datasets (train and val)\n",
    "x_train, y_train, x_val, y_val = serializing_datasets(train_dataset, valid_dataset)\n",
    "\n",
    "## Metric Functions: Confusion Matrix, Precision, Recall and F-1Scores\n",
    "# Confusion Matrix in Training\n",
    "plot_confusion_matrix(x_train, y_train, 'Greens', f'Confusion Matrix of Intrapersonal Training ', True)\n",
    "# Confusion Matrix in Testing\n",
    "intrapersonal_precision, intrapersonal_recall, intrapersonal_f1, intrapersonal_f1_micro  =\\\n",
    "\tplot_confusion_matrix(x_val, y_val, 'Blues', f'Confusion Matrix of Intrapersonal Validation ', True)"
   ],
   "id": "270273bd8ec87ea5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Log the best validation accuracy and loss\n",
    "wandb.log({\"intrapersonal_best_accuracy\":  best_val_accuracy,\n",
    "           \"intrapersonal_min_val_loss\": min_val_categ_crossentropy,\n",
    "           \"intrapersonal_precision\": intrapersonal_precision,\n",
    "           \"intrapersonal_recall\": intrapersonal_recall,\n",
    "           \"intrapersonal_f1\": intrapersonal_f1,\n",
    "           \"intrapersonal_f1_micro\": intrapersonal_f1_micro})\n",
    "wandb.finish()"
   ],
   "id": "72dbc327f0f15e3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Interpersonal Performance Test: Leave-One-Subject-Out Cross-Validation (LOSOCV)",
   "id": "f347efd75869fe80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_subjects = len(all_subjects_loo_data)\n",
    "best_loo_validation_per_subject = [];  min_loo_val_loss_per_subject = []\n",
    "all_interpersonal_precision = []; all_interpersonal_recall = []\n",
    "all_interpersonal_f1 = []; all_interpersonal_f1_micro = []"
   ],
   "id": "a9094367d62c1de3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# LOSOCV Training Loop\n",
    "for loo_subject in range(num_subjects):  # Change range to start from 0 to num_subjects-1\n",
    "\tprint(f\"Training with subject_{loo_subject+1} as a validation set\")\n",
    "\t\n",
    "\t# Deleting the old directories of the previously trained models\n",
    "\tclear_and_create_directory(\"model1\")\n",
    "\tclear_and_create_directory(\"histories\")\n",
    "\t\n",
    "\t# Initialising Wandb logging\n",
    "\twandb.init(project='Thesis', entity='firass-koli', config=config, name=f\"loo_test_subject_{loo_subject+1}\" , group=\"Inter\")\n",
    "\toptimizer = Adam(learning_rate=wandb.config.learning_rate)\n",
    "\t\n",
    "\t# Creating a new model\n",
    "\tmodel1 = model_creation()\n",
    "\t\t\n",
    "\t# Compiling the model\n",
    "\tmodel1.compile(loss=categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
    "\t\n",
    "\t# Defining checkpoint path correctly with adjusted loo_subject index\n",
    "\tcp = ModelCheckpoint('model1/best_model_epoch_{epoch:02d}_val_acc_{val_accuracy:.4f}.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "\n",
    "\t# Prepare training and validation datasets\n",
    "\tloo_valid_dataset = all_subjects_loo_data[loo_subject]\n",
    "\tloo_train_datasets = [d for i, d in enumerate(all_subjects_loo_data) if i != loo_subject]\n",
    "\ttrain_dataset = reduce(lambda x, y: x.concatenate(y), loo_train_datasets)  # Correct concatenation using lambda\n",
    "\n",
    "\t# Train the model with the correct validation dataset\n",
    "\twith tf.device('/gpu:0'):\n",
    "\t\thistory = model1.fit(train_dataset, validation_data=loo_valid_dataset,\n",
    "\t\t\t                     epochs=wandb.config.epochs, callbacks=[cp, scheduler, WandbMetricsLogger(log_freq=5)])\n",
    "\n",
    "\t# Log the best validation accuracy and loss for each subject\n",
    "\tbest_val_accuracy = round(max(history.history['val_accuracy']),3)*100\n",
    "\tmin_val_loss = round(min(history.history['val_loss']), 3)\n",
    "\t\n",
    "\tprint(f\"Best Validation Accuracy for using Subject_{loo_subject+1} as loo: {best_val_accuracy}%\")\n",
    "\tprint(f\"Minimum Validation Loss for using Subject_{loo_subject+1} as loo: {min_val_loss}\")\n",
    "\t\n",
    "\tbest_loo_validation_per_subject.append(best_val_accuracy)\n",
    "\tmin_loo_val_loss_per_subject.append(min_val_loss)\n",
    "\n",
    "\t# Saving history\n",
    "\thistory_name = f'histories/history(val_acc={best_val_accuracy}%,val_categ_crossentropy={min_val_loss})_loo_subject_{loo_subject+1}'\n",
    "\tnp.save(history_name + '.npy',history.history)\n",
    "\n",
    "\t## Plotting Loss and Accuracy Metrics over Epochs\n",
    "\tplotting_loss_and_accuracy_over_epochs(history_name, f'Loss and Accuracy over Epochs (Interpersonal with Subject_{loo_subject+1} as loo)')\n",
    "\n",
    "\t## Metric Functions: Confusion Matrix, Precision, Recall and F-1Scores\n",
    "\t# Serializing Datasets (train and val)\n",
    "\tx_train, y_train, x_val, y_val = serializing_datasets(train_dataset,loo_valid_dataset)\n",
    "\t# Confusion Matrix in Training\n",
    "\tplot_confusion_matrix(x_train, y_train, 'Greens', f'Confusion Matrix of Interpersonal Training with Subject_{loo_subject+1} as loo')\n",
    "\t# Confusion Matrix in Testing\n",
    "\tinterpersonal_precision, interpersonal_recall, interpersonal_f1, interpersonal_f1_micro  =\\\n",
    "\t\tplot_confusion_matrix(x_val, y_val, 'Blues', f'Confusion Matrix of Interpersonal LOO_Validation on Subject_{loo_subject+1}')\n",
    "\t\n",
    "\t# Log the best validation accuracy and loss\n",
    "\twandb.log({\"interpersonal_best_val_accuracy\":  best_val_accuracy,\n",
    "\t           \"interpersonal_min_val_loss\": min_val_loss,\n",
    "\t           \"interpersonal_precision\": interpersonal_precision,\n",
    "\t           \"interpersonal_recall\": interpersonal_recall,           \n",
    "\t           \"interpersonal_f1\": interpersonal_f1,\n",
    "\t           \"interpersonal_f1_micro\": interpersonal_f1_micro})\n",
    "\twandb.finish()\n",
    "\n",
    "\tall_interpersonal_precision.append(interpersonal_precision)\n",
    "\tall_interpersonal_recall.append(interpersonal_recall)\n",
    "\tall_interpersonal_f1.append(interpersonal_f1)\n",
    "\tall_interpersonal_f1_micro.append(interpersonal_f1_micro)"
   ],
   "id": "e6ab6618ce2adc22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_interpersonal_metrics_bar(all_interpersonal_precision, all_interpersonal_recall, all_interpersonal_f1_list, all_interpersonal_f1_micro_list):\n",
    "    # Helper function to ensure metrics are single values\n",
    "    def ensure_single_value(metrics):\n",
    "        if isinstance(metrics[0], (list, np.ndarray)):  # checks if the first element is a list or ndarray\n",
    "            metrics = [np.mean(metric) for metric in metrics]\n",
    "        return metrics\n",
    "\n",
    "    # Ensuring each metric is a single value by averaging if necessary\n",
    "    all_interpersonal_precision = ensure_single_value(all_interpersonal_precision)\n",
    "    all_interpersonal_recall = ensure_single_value(all_interpersonal_recall)\n",
    "    all_interpersonal_f1_list = ensure_single_value(all_interpersonal_f1_list)\n",
    "    all_interpersonal_f1_micro_list = ensure_single_value(all_interpersonal_f1_micro_list)\n",
    "\n",
    "    subjects_names = [f\"Subject_{i+1}\" for i in range(len(all_interpersonal_precision))]\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Interpersonal Metrics Comparison')\n",
    "    bar_width = 0.35\n",
    "\n",
    "    # Helper function to add value labels on top of bars\n",
    "    def add_value_labels(ax):\n",
    "        for i in ax.patches:\n",
    "            ax.text(i.get_x() + i.get_width() / 2, i.get_height(), \n",
    "                    round(i.get_height(), 2), ha='center', va='bottom')\n",
    "\n",
    "    # Plotting Precision\n",
    "    axs[0, 0].bar(subjects_names, all_interpersonal_precision, color='b', width=bar_width)\n",
    "    axs[0, 0].set_title(f'Interpersonal Precision (Overall Precision = {round(np.mean(all_interpersonal_precision), 3)})')\n",
    "    axs[0, 0].set_xlabel('Subjects')\n",
    "    axs[0, 0].set_ylabel('Precision')\n",
    "    axs[0, 0].set_ylim([0, 1])\n",
    "    add_value_labels(axs[0, 0])\n",
    "\n",
    "    # Plotting Recall\n",
    "    axs[0, 1].bar(subjects_names, all_interpersonal_recall, color='r', width=bar_width)\n",
    "    axs[0, 1].set_title(f'Interpersonal Recall (Overall Recall = {round(np.mean(all_interpersonal_recall), 3)})')\n",
    "    axs[0, 1].set_xlabel('Subjects')\n",
    "    axs[0, 1].set_ylabel('Recall')\n",
    "    axs[0, 1].set_ylim([0, 1])\n",
    "    add_value_labels(axs[0, 1])\n",
    "\n",
    "    # Plotting F1 Score\n",
    "    axs[1, 0].bar(subjects_names, all_interpersonal_f1_list, color='g', width=bar_width)\n",
    "    axs[1, 0].set_title(f'Interpersonal F1 Score (Overall F1 Score = {round(np.mean(all_interpersonal_f1_list), 3)})')\n",
    "    axs[1, 0].set_xlabel('Subjects')\n",
    "    axs[1, 0].set_ylabel('F1 Score')\n",
    "    axs[1, 0].set_ylim([0, 1])\n",
    "    add_value_labels(axs[1, 0])\n",
    "\n",
    "    # Plotting Micro F1 Score\n",
    "    axs[1, 1].bar(subjects_names, all_interpersonal_f1_micro_list, color='c', width=bar_width)\n",
    "    axs[1, 1].set_title(f'Interpersonal F1 Micro Score (Overall F1 Micro Score = {round(np.mean(all_interpersonal_f1_micro_list), 3)})')\n",
    "    axs[1, 1].set_xlabel('Subjects')\n",
    "    axs[1, 1].set_ylabel('F1 Micro Score')\n",
    "    axs[1, 1].set_ylim([0, 1])\n",
    "    add_value_labels(axs[1, 1])\n",
    "\n",
    "    # Ensure the directory for saving figures exists\n",
    "    output_dir = \"results_figures\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Improve layout and save the figure\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(f\"{output_dir}/interpersonal_test_scores.png\")\n",
    "    plt.show()"
   ],
   "id": "b72fd7d9d81b08b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plotting\n",
    "plot_interpersonal_metrics_bar(all_interpersonal_precision, all_interpersonal_recall, all_interpersonal_f1, all_interpersonal_f1_micro)"
   ],
   "id": "295cafd9c9d75191",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO:\n",
    "#  - 7ell el mochkel taa el text elli mich 9a3ed yodhher bil s7i7. \n",
    "#  - merge both notebooks! "
   ],
   "id": "12b5e9c69ed1c848",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5e40049a460874e3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
