{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Matrix and Dataframe Operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Tensorflow Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.losses import MeanAbsoluteError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import load_model\n",
    "import plotly\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import plotly.graph_objects as go"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Required Function (Pickle Save and Load)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_pickle(data,path):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(data, file, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        result = pickle.load(file)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading TFRecords (Training and Validation datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Iterate over the whole dataset to count records/samples (https://www.rustyrobotics.com/posts/tensorflow/tfdataset-record-count/)\n",
    "def countRecords(ds:tf.data.Dataset):\n",
    "    count = 0\n",
    "    if tf.executing_eagerly():\n",
    "        # TF v2 or v1 in eager mode\n",
    "        for r in ds:\n",
    "            count = count+1\n",
    "    else:\n",
    "        # TF v1 in non-eager mode\n",
    "        iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
    "        next_batch = iterator.get_next()\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            try:\n",
    "                while True:\n",
    "                    sess.run(next_batch)\n",
    "                    count = count+1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "    return count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def read_tfrecord(serialized_example, export_subject=False):\n",
    "    tfrecord_format = (\n",
    "        {\n",
    "            'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'feature': tf.io.FixedLenFeature([300], tf.float32),\n",
    "            'subject': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'burst':  tf.io.FixedLenFeature([], tf.int64)\n",
    "        }\n",
    "    )\n",
    "    example = tf.io.parse_single_example(serialized_example, tfrecord_format)\n",
    "    f = tf.reshape(example['feature'], [window,1])\n",
    "    f.set_shape([window, 1])\n",
    "    # One-hot encode the label to match the expected shape for categorical_crossentropy\n",
    "    label = tf.one_hot(example['label'], depth=14)\n",
    "    if export_subject:\n",
    "        return f, label, example['subject']\n",
    "    return f, label\n",
    "\n",
    "def get_dataset(tf_record_name):\n",
    "    dataset = tf.data.TFRecordDataset(tf_record_name)\n",
    "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n",
    "    dataset_samples = countRecords(dataset)\n",
    "#     print(\"Samples: \", dataset_samples)\n",
    "    dataset = dataset.shuffle(dataset_samples)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "window = 300\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 1024\n",
    "train_dataset = get_dataset('all_mixed_train.tfrecord')\n",
    "valid_dataset = get_dataset('all_mixed_val.tfrecord')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for feature, label in train_dataset.take(10):\n",
    "    print('label={}, feature={}'.format(label.shape, feature.shape))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CNN Layer 1: Layer-Wise-Unsupervised Pretraining"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cnn_layer_1 (Conv1D)        (None, 500, 32)           192       \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 500, 32)           0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 250, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_transpose_1 (Conv1D  (None, 500, 1)            161       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 500, 1)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 353 (1.38 KB)\n",
      "Trainable params: 353 (1.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Architecture of CNN1:\n",
    "\n",
    "# Parameters\n",
    "conv1D_1 = [32,5,1]  # number_filters,kernel_size and strides \n",
    "pool1D_1 = [2,2]     # pool_size and strides\n",
    "activation = \"leakyrelu\"\n",
    "\n",
    "# Definition\n",
    "model1 = Sequential()\n",
    "model1.add(InputLayer((window,1))) #InputLayer(BURST_WINDOW, N_CHANNELS)\n",
    "\n",
    "# CNN LAYER 1: DOWN BRANCH\n",
    "model1.add(Conv1D(filters=conv1D_1[0], kernel_size=conv1D_1[1], # TODO TRY WITH HIGHER KERNEL SIZE (ODD NUMBER!)\n",
    "                  strides=conv1D_1[2], padding='same', name='cnn_layer_1')) # activation='relu'))\n",
    "\n",
    "model1.add(LeakyReLU(alpha=0.1))\n",
    "model1.add(MaxPooling1D(pool_size=pool1D_1[0], strides=pool1D_1[1], padding='same'))\n",
    "\n",
    "# CNN LAYER 1: UP BRANCH\n",
    "model1.add(Conv1DTranspose(filters=1, kernel_size=conv1D_1[1], # TODO TRY WITH HIGHER KERNEL SIZE (ODD NUMBER!)\n",
    "                           strides=conv1D_1[2]+1, #2\n",
    "                           padding='same')) #activation='relu'))\n",
    "model1.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model1.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
