{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# 7 MUSCLES CLASSIFICATION + (INTER/INTRA)PERSONAL VALIDATION",
   "metadata": {
    "collapsed": false
   },
   "id": "9a6d23e0d0743caa"
  },
  {
   "cell_type": "code",
   "id": "37441250891c70ee",
   "metadata": {},
   "source": [
    "# Installation of BioSignalsNotebooks\n",
    "# %pip install biosignalsnotebooks\n",
    "# %pip install tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e2efabe2412d086",
   "metadata": {},
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# import biosignalsnotebooks as bsnb\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "# Tensorflow Model\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.src.layers import Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.src.callbacks import ModelCheckpoint\n",
    "from keras.src.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.layers import InputLayer, Conv1D, LeakyReLU, MaxPooling1D, LSTM #, PReLU \n",
    "\n",
    "# Weight and Biases\n",
    "import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "from wandb.keras import WandbMetricsLogger #, WandbModelCheckpoint\n",
    "\n",
    "# Model Metrics\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Creating the sEMG Signal Dataframes from the CSV files",
   "metadata": {
    "collapsed": false
   },
   "id": "fed287a164bc3c08"
  },
  {
   "cell_type": "code",
   "id": "e4c2a1c3ac0dd0a3",
   "metadata": {},
   "source": [
    "# # Function to create a dataframe to store the data of every subject across all trials \n",
    "# def subject_df_creator(subject_id, muscles_of_interest):\n",
    "#     trials_dfs_list = []\n",
    "#     output_msg=[]\n",
    "# \n",
    "#     for trial_number in tqdm(range(1, 51), desc=f\"Concatenating trial Files for Subject AB{subject_id}\"):\n",
    "#         filename = f\"5362627/AB{subject_id}/AB{subject_id}/Raw/AB{subject_id}_Circuit_0{trial_number:02d}_raw.csv\"\n",
    "#         if not os.path.exists(filename):  # Check if the file exists\n",
    "#             output_msg.append(f\"0{trial_number:02d}\")\n",
    "#             continue\n",
    "# \n",
    "#         df_trial = pd.read_csv(filename)\n",
    "#         df_trial_combined = pd.DataFrame()\n",
    "# \n",
    "#         # Concatenate 'Right_' and 'Left_' values for each muscle of interest\n",
    "#         for i in range(len(muscles_of_interest)):\n",
    "#             df_trial_combined[muscles_of_interest[i]] = pd.concat([df_trial['Right_'+muscles_of_interest[i]], df_trial['Left_'+muscles_of_interest[i]]], ignore_index=True)\n",
    "# \n",
    "#         trials_dfs_list.append(df_trial_combined)\n",
    "# \n",
    "#     # Concatenate all DataFrames in the list along the rows axis\n",
    "#     merged_df = pd.concat(trials_dfs_list, ignore_index=True)\n",
    "#     if output_msg:\n",
    "#         print(f\"{len(output_msg)} Files do not exist:\", output_msg)\n",
    "#     return merged_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Defining important lists\n",
    "subjects = [\"156\", \"185\", \"186\", \"188\", \"189\", \"190\", \"191\", \"192\", \"193\", \"194\"]\n",
    "muscles = ['TA', 'MG', 'SOL', 'BF', 'ST', 'VL', 'RF']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3a82d6844b895f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "674c2f5bcc6b8187",
   "metadata": {},
   "source": [
    "# # Creating dataframes to save subject-specific data\n",
    "# df_subject_156 = subject_df_creator(\"156\", muscles)\n",
    "# df_subject_185 = subject_df_creator(\"185\", muscles)\n",
    "# df_subject_186 = subject_df_creator(\"186\", muscles)\n",
    "# df_subject_188 = subject_df_creator(\"188\", muscles)\n",
    "# df_subject_189 = subject_df_creator(\"189\", muscles)\n",
    "# df_subject_190 = subject_df_creator(\"190\", muscles)\n",
    "# df_subject_191 = subject_df_creator(\"191\", muscles)\n",
    "# df_subject_192 = subject_df_creator(\"192\", muscles)\n",
    "# df_subject_193 = subject_df_creator(\"193\", muscles)\n",
    "# df_subject_194 = subject_df_creator(\"194\", muscles)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# # Saving the Dataframes into pickle objects to save time\n",
    "# df_subject_156.to_pickle('pickled_dataframes/df_subject_156.pkl')\n",
    "# df_subject_185.to_pickle('pickled_dataframes/df_subject_185.pkl')\n",
    "# df_subject_186.to_pickle('pickled_dataframes/df_subject_186.pkl')\n",
    "# df_subject_188.to_pickle('pickled_dataframes/df_subject_188.pkl')\n",
    "# df_subject_189.to_pickle('pickled_dataframes/df_subject_189.pkl')\n",
    "# df_subject_190.to_pickle('pickled_dataframes/df_subject_190.pkl')\n",
    "# df_subject_191.to_pickle('pickled_dataframes/df_subject_191.pkl')\n",
    "# df_subject_192.to_pickle('pickled_dataframes/df_subject_192.pkl')\n",
    "# df_subject_193.to_pickle('pickled_dataframes/df_subject_193.pkl')\n",
    "# df_subject_194.to_pickle('pickled_dataframes/df_subject_194.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71aa6e6992196281",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Loading pickle files into DataFrames\n",
    "df_subject_156 = pd.read_pickle('pickled_dataframes/df_subject_156.pkl')\n",
    "df_subject_185 = pd.read_pickle('pickled_dataframes/df_subject_185.pkl')\n",
    "df_subject_186 = pd.read_pickle('pickled_dataframes/df_subject_186.pkl')\n",
    "df_subject_188 = pd.read_pickle('pickled_dataframes/df_subject_188.pkl')\n",
    "df_subject_189 = pd.read_pickle('pickled_dataframes/df_subject_189.pkl')\n",
    "df_subject_190 = pd.read_pickle('pickled_dataframes/df_subject_190.pkl')\n",
    "df_subject_191 = pd.read_pickle('pickled_dataframes/df_subject_191.pkl')\n",
    "df_subject_192 = pd.read_pickle('pickled_dataframes/df_subject_192.pkl')\n",
    "df_subject_193 = pd.read_pickle('pickled_dataframes/df_subject_193.pkl')\n",
    "df_subject_194 = pd.read_pickle('pickled_dataframes/df_subject_194.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9418bcf6748f92f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "477e4fb02daa7f93",
   "metadata": {},
   "source": [
    "# Merging all the subject dataframes into one\n",
    "list_of_all_subjects_dfs = [df_subject_156, df_subject_185, df_subject_186, df_subject_188, df_subject_189,\n",
    "                            df_subject_190, df_subject_191, df_subject_192, df_subject_193, df_subject_194]\n",
    "\n",
    "df_all_subjects = pd.concat(list_of_all_subjects_dfs, ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "885a9899ab761bf2",
   "metadata": {},
   "source": [
    "df_all_subjects"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "44146ec683a0fc26",
   "metadata": {},
   "source": "## EMG Signal Visualisation "
  },
  {
   "cell_type": "code",
   "id": "2423d44728d53dbd",
   "metadata": {},
   "source": [
    "# # adding figures and traces\n",
    "# fig1 = go.Figure()\n",
    "# fig1.add_trace(go.Scatter(x=df_all_subjects.index/1000, y=df_all_subjects['TA'][60000:90000]))\n",
    "# fig1.update_layout( title=\"sEMG Signal: Sitting Vs Contraction Bursts Vs Rest\", xaxis_title=\"Time (s)\",\n",
    "#                     yaxis_title=\"sEMG Activity (V)\", margin=dict(l=50, r=50, b=50, t=50, pad=4),\n",
    "#                     autosize=False, width=800, height=301)\n",
    "# # plotting\n",
    "# fig1.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e77c58d1388c023",
   "metadata": {},
   "source": "## EMG Signal Analysis"
  },
  {
   "cell_type": "code",
   "id": "b5bcc96368fa6ee6",
   "metadata": {},
   "source": [
    "# Studying mean, sigma and variance of the 2 Muscles\n",
    "df_analysis = pd.DataFrame()\n",
    "df_analysis['Mean'] = df_all_subjects.mean()\n",
    "df_analysis['Std'] = df_all_subjects.std()\n",
    "df_analysis['Var'] = df_all_subjects.var()\n",
    "df_analysis"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Burst Detection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81ea87cc5fdbb0f1"
  },
  {
   "cell_type": "code",
   "id": "88b8ed3bff3a2166",
   "metadata": {},
   "source": [
    "# # Saving the detected bursts for every muscle \n",
    "# sr = 1000 # sample rate = 1000Hz\n",
    "# sl = 20 # smooth level (Size of sliding window used during the moving average process) #used to be 40\n",
    "# th = 10 # threshold (To cover activation)\n",
    "# \n",
    "# # Initializing lists\n",
    "# detected_bursts_TA = [] ; detected_bursts_MG = [] ; detected_bursts_SOL= []\n",
    "# detected_bursts_BF = [] ; detected_bursts_ST = [] ; detected_bursts_VL = []\n",
    "# detected_bursts_RF = []\n",
    "# \n",
    "# pbar = tqdm(total=len(muscles)*len(list_of_all_subjects_dfs), desc=\"All Subjects Burst Detection Progress\", unit= \"Muscle\")\n",
    "# for df_subject in list_of_all_subjects_dfs:\n",
    "#     ## TA\n",
    "#     detected_bursts_TA.append(bsnb.detect_emg_activations(emg_signal=df_subject['TA'], sample_rate=sr, smooth_level=sl,\n",
    "#                                                           threshold_level=th, time_units=True, device='CH0', plot_result= False))\n",
    "#     pbar.update(1)  # Update progress bar\n",
    "#     ## MG\n",
    "#     detected_bursts_MG.append(bsnb.detect_emg_activations(emg_signal=df_subject['MG'], sample_rate=sr, smooth_level=sl,\n",
    "#                                                           threshold_level=th, time_units=True, device='CH0', plot_result= False))\n",
    "#     pbar.update(1)\n",
    "#     ## SOL\n",
    "#     detected_bursts_SOL.append(bsnb.detect_emg_activations(emg_signal=df_subject['SOL'], sample_rate=sr, smooth_level=sl,\n",
    "#                                                            threshold_level=th, time_units=True, device='CH0', plot_result= False))\n",
    "#     pbar.update(1)\n",
    "#     ## BF\n",
    "#     detected_bursts_BF.append(bsnb.detect_emg_activations(emg_signal=df_subject['BF'], sample_rate=sr, smooth_level=sl,\n",
    "#                                                           threshold_level=th, time_units=True, device='CH0', plot_result= False))\n",
    "#     pbar.update(1)\n",
    "#     ## ST\n",
    "#     detected_bursts_ST.append(bsnb.detect_emg_activations(emg_signal=df_subject['ST'], sample_rate=sr, smooth_level=sl,\n",
    "#                                                           threshold_level=th, time_units=True, device='CH0', plot_result= False))\n",
    "#     pbar.update(1)\n",
    "#     ## VL\n",
    "#     detected_bursts_VL.append(bsnb.detect_emg_activations(emg_signal=df_subject['VL'], sample_rate=sr, smooth_level=sl,\n",
    "#                                                           threshold_level=th, time_units=True, device='CH0', plot_result= False))\n",
    "#     pbar.update(1)\n",
    "#     ## RF\n",
    "#     detected_bursts_RF.append(bsnb.detect_emg_activations(emg_signal=df_subject['RF'], sample_rate=sr, smooth_level=sl,\n",
    "#                                                           threshold_level=th, time_units=True, device='CH0', plot_result= False))\n",
    "#     pbar.update(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# # Creating the pickles to save the burst detection results (saves 29 minutes)\n",
    "# with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_TA.pkl', 'wb') as f: pickle.dump(detected_bursts_TA, f)\n",
    "# with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_MG.pkl', 'wb') as f: pickle.dump(detected_bursts_MG, f)\n",
    "# with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_SOL.pkl','wb') as f: pickle.dump(detected_bursts_SOL,f)\n",
    "# with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_BF.pkl', 'wb') as f: pickle.dump(detected_bursts_BF, f)\n",
    "# with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_ST.pkl', 'wb') as f: pickle.dump(detected_bursts_ST, f)\n",
    "# with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_VL.pkl', 'wb') as f: pickle.dump(detected_bursts_VL, f)\n",
    "# with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_RF.pkl', 'wb') as f: pickle.dump(detected_bursts_RF, f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32689628ce37e44c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Loading the pickles\n",
    "pbar = tqdm(total=len(muscles), desc=\"Burst Detection Loading Progress\", unit= \"Muscle\")\n",
    "with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_TA.pkl', 'rb') as f: detected_bursts_TA = pickle.load(f)\n",
    "pbar.update(1)\n",
    "with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_MG.pkl', 'rb') as f: detected_bursts_MG = pickle.load(f)\n",
    "pbar.update(1)\n",
    "with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_SOL.pkl','rb') as f: detected_bursts_SOL= pickle.load(f)\n",
    "pbar.update(1)\n",
    "with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_BF.pkl', 'rb') as f: detected_bursts_BF = pickle.load(f)\n",
    "pbar.update(1)\n",
    "with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_ST.pkl', 'rb') as f: detected_bursts_ST = pickle.load(f)\n",
    "pbar.update(1)\n",
    "with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_VL.pkl', 'rb') as f: detected_bursts_VL = pickle.load(f)\n",
    "pbar.update(1)\n",
    "with open('pickled_detected_bursts/7_muscles_all_subjects_detected_bursts_RF.pkl', 'rb') as f: detected_bursts_RF = pickle.load(f)\n",
    "pbar.update(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66629be7075b0eda",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# pd.DataFrame(detected_bursts_SOL[:][:10]).transpose()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6511bb972b75fcd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Detected Bursts Visualisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b633b4b9cdb961f4"
  },
  {
   "cell_type": "code",
   "source": [
    "# # Visualising the EMG Burst Detection for SOL\n",
    "# plot_duration = 40000 # time in milliseconds\n",
    "# bsnb.detect_emg_activations(emg_signal = df_all_subjects['SOL'][:plot_duration], sample_rate = sr, smooth_level=sl, threshold_level=th, time_units=True, device='CH0', plot_result= True)\n",
    "# print('')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "996120a536564717",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4f73c254a64ba3c",
   "metadata": {},
   "source": [
    "# # Visualizing First Activations -> SEE CAPTURED WINDOW WITH RESPECT OF IDENTIFIED ACTIVATION\n",
    "# duration = 8000\n",
    "# shift = 2000\n",
    "# number_bursts_to_plot = 1\n",
    "# \n",
    "# plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "# fig = plt.figure()\n",
    "# \n",
    "# plt.plot(df_all_subjects['TA'][:duration], color=\"cornflowerblue\")\n",
    "# for i in range(number_bursts_to_plot): # Plot first N bursts\n",
    "#     plt.axvline(detected_bursts_TA[0][i]*1000,color='red', label=\"Detected Burst Region\") # ONSET VERTICAL LINE\n",
    "#     plt.axvline(detected_bursts_TA[1][i]*1000,color='red') # OFFSET VERTICAL LINE\n",
    "#     plt.axvline(detected_bursts_TA[0][i]*1000+400,color='black', label=\"Onset Window (300ms)\") # ONSET VERTICAL LINE CORRECTED (START WINDOW)\n",
    "#     plt.axvline(detected_bursts_TA[0][i]*1000-100,color='black') # VERTICAL LINE (END WINDOW)\n",
    "#     \n",
    "# plt.legend(loc=\"upper left\")\n",
    "# plt.xlim(shift,duration)\n",
    "# plt.grid()\n",
    "# plt.xlabel('Time (ms)', fontsize=10)\n",
    "# plt.ylabel('sEMG Intensity (V)', fontsize=10)\n",
    "# \n",
    "# # plt.savefig(\"Window.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f99ecb5e20d3e887",
   "metadata": {},
   "source": [
    "# # adding figures and traces\n",
    "# fig1 = go.Figure()\n",
    "# fig1.add_trace(go.Scatter(x = df_all_subjects.index/1000 , y=df_all_subjects['Left_TA'][:10000]))\n",
    "# \n",
    "# # formatting the plot\n",
    "# fig1.update_layout(autosize=True, title=\"sEMG Signal: Detected burst and corrected onset window\",\n",
    "#                    xaxis_title=\"Time (s)\", yaxis_title=\"sEMG Activity (V)\", margin=dict(l=50, r=50, b=50, t=50, pad=4))\n",
    "# \n",
    "# fig1.add_vrect(x0=detected_bursts_TA[0][0], x1=detected_bursts_left_TA[1][0], row=\"all\", col=1,\n",
    "#                annotation_text=\"Detected Burst\", annotation_position=\"top right\", fillcolor=\"gray\",\n",
    "#                opacity=0.25, line_width=0)\n",
    "# \n",
    "# fig1.add_vline(x=detected_bursts_left_TA[0][0]+0.4,line_width=1.5, line_dash=\"dot\", line_color=\"red\")\n",
    "# fig1.add_vline(x=detected_bursts_left_TA[0][0]-0.1,line_width=1.5, line_dash=\"dot\", line_color=\"red\",\n",
    "#                annotation_text=\"Onset Window\",annotation_position=\"bottom right\")\n",
    "# \n",
    "# # fig1.update_xaxes(range=[7.5, 20000/1000])\n",
    "# # fig1.update_yaxes(range=[-2, 2])\n",
    "# fig1.update_layout(autosize=False, width=800, height=301)\n",
    "# # plotting\n",
    "# fig1.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3fc9b4c4fbb786b",
   "metadata": {},
   "source": [
    "# # adding figures and traces\n",
    "# fig1 = go.Figure()\n",
    "# fig1.add_trace(go.Scatter(x= df_all_subjects.index/1000, y=df_all_subjects['TA']))\n",
    "# # formatting the plot\n",
    "# fig1.update_layout(autosize=True, title=\"sEMG Signal: Detection of Activation Bursts\",\n",
    "#                    xaxis_title=\"Time (s)\", yaxis_title=\"sEMG Activity (V)\",\n",
    "#                    margin=dict(l=50, r=50, b=50, t=50, pad=4))\n",
    "# \n",
    "# for i in range(len(detected_bursts_TA[0])):\n",
    "#     fig1.add_vrect(x0=detected_bursts_TA[0][i], x1=detected_bursts_TA[1][i], row=\"all\", col=1,\n",
    "#                    annotation_text=\"Detected Burst\", annotation_position=\"top right\",\n",
    "#                    fillcolor=\"black\", opacity=0.25, line_width=0)\n",
    "# \n",
    "# # fig1.update_xaxes(range=[30, 60])\n",
    "# fig1.update_layout(autosize=False, width=800, height=301)\n",
    "# # plotting\n",
    "# fig1.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Detected Bursts Analysis "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "269ba9449138c886"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Number of Detected Bursts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaf5b33d7626e0e8"
  },
  {
   "cell_type": "code",
   "source": [
    "# Defining Variables to store the total number of bursts for every muscle \n",
    "tot_bursts_TA = []; tot_bursts_MG = []; tot_bursts_SOL= []; tot_bursts_BF = []\n",
    "tot_bursts_ST = []; tot_bursts_VL = []; tot_bursts_RF = []\n",
    "burst_count_list_for_printing = []\n",
    "\n",
    "# Calculating the total number of bursts per muscle\n",
    "for subject_idx, subject in enumerate(subjects):\n",
    "    tot_bursts_TA.append(len(detected_bursts_TA[subject_idx][0]))\n",
    "    tot_bursts_MG.append(len(detected_bursts_MG[subject_idx][0]))\n",
    "    tot_bursts_SOL.append(len(detected_bursts_SOL[subject_idx][0]))\n",
    "    tot_bursts_BF.append(len(detected_bursts_BF[subject_idx][0]))\n",
    "    tot_bursts_ST.append(len(detected_bursts_ST[subject_idx][0]))\n",
    "    tot_bursts_VL.append(len(detected_bursts_VL[subject_idx][0]))\n",
    "    tot_bursts_RF.append(len(detected_bursts_RF[subject_idx][0]))\n",
    "    # Saving the results in a list for fancy printing\n",
    "    burst_count_list_for_printing.append([subject, tot_bursts_TA[subject_idx], tot_bursts_MG[subject_idx], tot_bursts_SOL[subject_idx],\n",
    "                                          tot_bursts_BF[subject_idx], tot_bursts_ST[subject_idx], tot_bursts_VL[subject_idx],\n",
    "                                          tot_bursts_RF[subject_idx]])\n",
    "\n",
    "# Adding the total row to the printing\n",
    "burst_count_list_for_printing.append([])\n",
    "burst_count_list_for_printing.append(['---'])\n",
    "burst_count_list_for_printing.append(['Total Sum', sum(tot_bursts_TA), sum(tot_bursts_MG), sum(tot_bursts_SOL), sum(tot_bursts_BF), sum(tot_bursts_ST), sum(tot_bursts_VL), sum(tot_bursts_RF)])\n",
    "\n",
    "# Printing the table\n",
    "print(\"Number of Muscle Bursts Per Subject Per Muscle:\\n\")\n",
    "headers = [\"Subject\", \"TA Bursts\", \"MG Bursts\", \"SOL Bursts\", \"BF Bursts\", \"ST Bursts\", \"VL Bursts\", \"RF Bursts\"]\n",
    "print(tabulate(burst_count_list_for_printing, headers=headers))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba4fbe0e78e31449",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Longest Detected Bursts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b26b447d9810906"
  },
  {
   "cell_type": "code",
   "source": [
    "# Lists to store longest burst per muscle for all subjects\n",
    "longest_bursts_TA = []; longest_bursts_MG = []; longest_bursts_SOL= []\n",
    "longest_bursts_BF = []; longest_bursts_ST = []; longest_bursts_VL = []\n",
    "longest_bursts_RF = []\n",
    "longest_burst_list_for_printing = []\n",
    "\n",
    "# Calculating the longest burst per muscle\n",
    "for subject_idx, subject in enumerate(subjects):\n",
    "    longest_bursts_TA.append(max(np.array(detected_bursts_TA[subject_idx][1])-np.array(detected_bursts_TA[subject_idx][0])))\n",
    "    longest_bursts_MG.append(max(np.array(detected_bursts_MG[subject_idx][1])-np.array(detected_bursts_MG[subject_idx][0])))\n",
    "    longest_bursts_SOL.append(max(np.array(detected_bursts_SOL[subject_idx][1])-np.array(detected_bursts_SOL[subject_idx][0])))\n",
    "    longest_bursts_BF.append(max(np.array(detected_bursts_BF[subject_idx][1])-np.array(detected_bursts_BF[subject_idx][0])))\n",
    "    longest_bursts_ST.append(max(np.array(detected_bursts_ST[subject_idx][1])-np.array(detected_bursts_ST[subject_idx][0])))\n",
    "    longest_bursts_VL.append(max(np.array(detected_bursts_VL[subject_idx][1])-np.array(detected_bursts_VL[subject_idx][0])))\n",
    "    longest_bursts_RF.append(max(np.array(detected_bursts_RF[subject_idx][1])-np.array(detected_bursts_RF[subject_idx][0])))\n",
    "    # Saving the results in a list for printing\n",
    "    longest_burst_list_for_printing.append([subject, longest_bursts_TA[subject_idx],longest_bursts_MG[subject_idx],\n",
    "                                            longest_bursts_SOL[subject_idx],longest_bursts_BF[subject_idx],\n",
    "                                            longest_bursts_ST[subject_idx],longest_bursts_VL[subject_idx],\n",
    "                                            longest_bursts_RF[subject_idx]])\n",
    "\n",
    "# Adding the total row to the printing\n",
    "longest_burst_list_for_printing.append([])\n",
    "longest_burst_list_for_printing.append(['---'])\n",
    "longest_burst_list_for_printing.append(['Longest Burst', max(longest_bursts_TA), max(longest_bursts_MG),\n",
    "                                        max(longest_bursts_SOL), max(longest_bursts_BF),\n",
    "                                        max(longest_bursts_ST), max(longest_bursts_VL),\n",
    "                                        max(longest_bursts_RF)])\n",
    "\n",
    "# Printing the table\n",
    "print(\"Longest Burst Per Subject Per Muscle:\\n\")\n",
    "headers = [\"Subject\", \"TA (s)\", \"MG (s)\", \"SOL (s)\", \"BF (s)\", \"ST (s)\", \"VL (s)\", \"RF (s)\"]\n",
    "print(tabulate(longest_burst_list_for_printing, headers=headers))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2097a7200037a085",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Average Detected Bursts Lengths"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5900907130d06e68"
  },
  {
   "cell_type": "code",
   "id": "403d50b477df1e2f",
   "metadata": {},
   "source": [
    "# Lists to store average burst length per muscle for all subjects\n",
    "average_burst_lengths_TA = [] ; average_burst_lengths_MG = [] ; average_burst_lengths_SOL= []\n",
    "average_burst_lengths_BF = [] ; average_burst_lengths_ST = [] ; average_burst_lengths_VL = []\n",
    "average_burst_lengths_RF = []\n",
    "avg_burst_len_list_for_printing = []\n",
    "\n",
    "\n",
    "# Calculating average burst length per muscle\n",
    "for subject_idx, subject in enumerate(subjects):\n",
    "    average_burst_lengths_TA.append(np.mean(np.array(detected_bursts_TA[subject_idx][1])-np.array(detected_bursts_TA[subject_idx][0])))\n",
    "    average_burst_lengths_MG.append(np.mean(np.array(detected_bursts_MG[subject_idx][1])-np.array(detected_bursts_MG[subject_idx][0])))\n",
    "    average_burst_lengths_SOL.append(np.mean(np.array(detected_bursts_SOL[subject_idx][1])-np.array(detected_bursts_SOL[subject_idx][0])))\n",
    "    average_burst_lengths_BF.append(np.mean(np.array(detected_bursts_BF[subject_idx][1])-np.array(detected_bursts_BF[subject_idx][0])))\n",
    "    average_burst_lengths_ST.append(np.mean(np.array(detected_bursts_ST[subject_idx][1])-np.array(detected_bursts_ST[subject_idx][0])))\n",
    "    average_burst_lengths_VL.append(np.mean(np.array(detected_bursts_VL[subject_idx][1])-np.array(detected_bursts_VL[subject_idx][0])))\n",
    "    average_burst_lengths_RF.append(np.mean(np.array(detected_bursts_RF[subject_idx][1])-np.array(detected_bursts_RF[subject_idx][0])))\n",
    "    # Saving the results in a list for fancy printing\n",
    "    avg_burst_len_list_for_printing.append([subject,\n",
    "                                            round(np.mean(average_burst_lengths_TA[subject_idx]), 3),\n",
    "                                            round(np.mean(average_burst_lengths_MG[subject_idx]), 3),\n",
    "                                            round(np.mean(average_burst_lengths_SOL[subject_idx]), 3),\n",
    "                                            round(np.mean(average_burst_lengths_BF[subject_idx]), 3),\n",
    "                                            round(np.mean(average_burst_lengths_ST[subject_idx]), 3),\n",
    "                                            round(np.mean(average_burst_lengths_VL[subject_idx]), 3),\n",
    "                                            round(np.mean(average_burst_lengths_RF[subject_idx]), 3)])\n",
    "\n",
    "# Adding the total row to the printing\n",
    "avg_burst_len_list_for_printing.append([])\n",
    "avg_burst_len_list_for_printing.append(['---'])\n",
    "avg_burst_len_list_for_printing.append(['Avg. Len', round(np.mean(average_burst_lengths_TA), 3), round(np.mean(average_burst_lengths_MG), 3),\n",
    "                                        round(np.mean(average_burst_lengths_SOL), 3), round(np.mean(average_burst_lengths_BF), 3),\n",
    "                                        round(np.mean(average_burst_lengths_ST), 3), round(np.mean(average_burst_lengths_VL), 3),\n",
    "                                        round(np.mean(average_burst_lengths_RF), 3)])\n",
    "\n",
    "# Printing the table\n",
    "print(\"Average Burst Length Per Subject Per Muscle:\\n\")\n",
    "headers = [\"Subject\", \"TA Avg.Len (s)\", \"MG Avg.Len (s)\", \"SOL Avg.Len (s)\", \"BF Avg.Len (s)\", \"ST Avg.Len (s)\", \"VL Avg.Len (s)\", \"RF Avg.Len (s)\"]\n",
    "print(tabulate(avg_burst_len_list_for_printing, headers=headers))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Histogram: Detected Bursts Lengths"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdff3aa7d77dc609"
  },
  {
   "cell_type": "code",
   "source": [
    "# Adjust subplot indexing\n",
    "f, a = plt.subplots(4, 2)\n",
    "f.set_size_inches(10, 20)\n",
    "a = a.ravel()\n",
    "bin_edges = np.arange(start=0, stop=2, step=0.05)  # Example for bins of width 0.05\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'black', 'pink', 'cyan', 'lime', 'yellow']\n",
    "\n",
    "for subject_idx, subject in enumerate(subjects):\n",
    "    subject = \"subject_\" + subject\n",
    "    a[0].hist(np.array(detected_bursts_TA[subject_idx][1])-np.array(detected_bursts_TA[subject_idx][0]), bins=bin_edges, alpha=0.5, label=subject, color=colors[subject_idx])\n",
    "    a[1].hist(np.array(detected_bursts_MG[subject_idx][1])-np.array(detected_bursts_MG[subject_idx][0]), bins=bin_edges, alpha=0.5, label=subject, color=colors[subject_idx])\n",
    "    a[2].hist(np.array(detected_bursts_SOL[subject_idx][1])-np.array(detected_bursts_SOL[subject_idx][0]),bins=bin_edges, alpha=0.5, label=subject, color=colors[subject_idx])\n",
    "    a[3].hist(np.array(detected_bursts_BF[subject_idx][1])-np.array(detected_bursts_BF[subject_idx][0]), bins=bin_edges, alpha=0.5, label=subject, color=colors[subject_idx])\n",
    "    a[4].hist(np.array(detected_bursts_ST[subject_idx][1])-np.array(detected_bursts_ST[subject_idx][0]), bins=bin_edges, alpha=0.5, label=subject, color=colors[subject_idx])\n",
    "    a[5].hist(np.array(detected_bursts_VL[subject_idx][1])-np.array(detected_bursts_VL[subject_idx][0]), bins=bin_edges, alpha=0.5, label=subject, color=colors[subject_idx])\n",
    "    a[6].hist(np.array(detected_bursts_RF[subject_idx][1])-np.array(detected_bursts_RF[subject_idx][0]), bins=bin_edges, alpha=0.5, label=subject, color=colors[subject_idx])\n",
    "\n",
    "# Set legends, titles, and labels for each subplot\n",
    "for muscle_idx, muscle in enumerate(muscles):\n",
    "    a[muscle_idx].legend(loc='upper right')\n",
    "    a[muscle_idx].set_title('Histogram Burst Duration: ' + muscle)\n",
    "    a[muscle_idx].set_xlabel(\"Burst Duration (seconds)\")\n",
    "    a[muscle_idx].set_ylabel(\"Occurrences\")\n",
    "    a[muscle_idx].set_xlim([0, 2])  # Remove to see how bad the burst detection is \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bde553b753a37fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Leave one out: Separating a Subject"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "220c511dd7f13320"
  },
  {
   "cell_type": "code",
   "source": "leave_one_out = 2 # Specify which one to leave out (from 0 to 9)",
   "metadata": {
    "collapsed": false
   },
   "id": "a4169ca5305ff779",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Saving the LOO Bursts\n",
    "loo_detected_bursts_TA = [detected_bursts_TA.pop(leave_one_out)]\n",
    "loo_detected_bursts_MG = [detected_bursts_MG.pop(leave_one_out)]\n",
    "loo_detected_bursts_SOL= [detected_bursts_SOL.pop(leave_one_out)]\n",
    "loo_detected_bursts_BF = [detected_bursts_BF.pop(leave_one_out)]\n",
    "loo_detected_bursts_ST = [detected_bursts_ST.pop(leave_one_out)]\n",
    "loo_detected_bursts_VL = [detected_bursts_VL.pop(leave_one_out)]\n",
    "loo_detected_bursts_RF = [detected_bursts_RF.pop(leave_one_out)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d588ecd4e2b17036",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Saving the LOO EMG signal\n",
    "loo_emg_signal_TA  = list(list_of_all_subjects_dfs[leave_one_out]['TA'])\n",
    "loo_emg_signal_MG  = list(list_of_all_subjects_dfs[leave_one_out]['MG'])\n",
    "loo_emg_signal_SOL = list(list_of_all_subjects_dfs[leave_one_out]['SOL'])\n",
    "loo_emg_signal_BF  = list(list_of_all_subjects_dfs[leave_one_out]['BF'])\n",
    "loo_emg_signal_ST  = list(list_of_all_subjects_dfs[leave_one_out]['ST'])\n",
    "loo_emg_signal_VL  = list(list_of_all_subjects_dfs[leave_one_out]['VL'])\n",
    "loo_emg_signal_RF  = list(list_of_all_subjects_dfs[leave_one_out]['RF'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "322b1dac5b4becd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Removing the LOO from the training\n",
    "loo_subject_id = subjects[leave_one_out]\n",
    "del subjects[leave_one_out]\n",
    "del list_of_all_subjects_dfs[leave_one_out]\n",
    "df_all_subjects = pd.concat(list_of_all_subjects_dfs, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a1d0b55626daa02",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Verifying the LOO process\n",
    "print(f\"Subject AB{loo_subject_id} was excluded from the training!\\n\"\n",
    "      f\"{len(list_of_all_subjects_dfs)} subjects remaining for training!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e04082636b654f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extracting Bursts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "969a614a9718648"
  },
  {
   "cell_type": "markdown",
   "source": "### Method: Preserving Onset and Window = 700ms",
   "metadata": {
    "collapsed": false
   },
   "id": "7b4bb78f5c6e1c60"
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_burst_windows(muscle_emg_signal, onset_list, window_size, left_shift_value, muscle_name):\n",
    "    sampling_rate = 1000\n",
    "    all_subjects_burst_samples = []\n",
    "    for subject_index in tqdm(range(len(onset_list)), desc=f\"Extracting Bursts for {muscle_name}\"):\n",
    "        current_subject_bursts = []\n",
    "        ii = -1\n",
    "        for onset in onset_list[subject_index][0]:\n",
    "            ii += 1\n",
    "            onset_ms = int(onset * sampling_rate) - left_shift_value\n",
    "            current_sample_window = []\n",
    "            if onset != onset_list[subject_index][0][-1]:\n",
    "                if (onset_ms + window_size) < (onset_list[subject_index][0][ii + 1] * 1000):\n",
    "                    for time_step in range(window_size):\n",
    "                        current_sample_window.append(muscle_emg_signal[onset_ms + time_step])\n",
    "            else:\n",
    "                for time_step in range(window_size):\n",
    "                    current_sample_window.append(muscle_emg_signal[onset_ms + time_step])\n",
    "\n",
    "            if current_sample_window:\n",
    "                current_sample_window -= np.mean(current_sample_window)\n",
    "                current_subject_bursts.append(current_sample_window)\n",
    "        all_subjects_burst_samples.append(current_subject_bursts)\n",
    "    return all_subjects_burst_samples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d5af44622972382",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "window = 700 # in ms (total window size)\n",
    "left_shift = 100 # in ms (left shift from detected onset). See detected onset on the vertical red lines in the plots above\n",
    "\n",
    "fixed_bursts_TA = extract_burst_windows(df_all_subjects['TA'], detected_bursts_TA, window, left_shift, 'TA')\n",
    "fixed_bursts_MG = extract_burst_windows(df_all_subjects['MG'], detected_bursts_MG, window, left_shift, 'MG')\n",
    "fixed_bursts_SOL= extract_burst_windows(df_all_subjects['SOL'],detected_bursts_SOL,window, left_shift, 'SOL')\n",
    "fixed_bursts_BF = extract_burst_windows(df_all_subjects['BF'], detected_bursts_BF, window, left_shift, 'BF')\n",
    "fixed_bursts_ST = extract_burst_windows(df_all_subjects['ST'], detected_bursts_ST, window, left_shift, 'ST')\n",
    "fixed_bursts_VL = extract_burst_windows(df_all_subjects['VL'], detected_bursts_VL, window, left_shift, 'VL')\n",
    "fixed_bursts_RF = extract_burst_windows(df_all_subjects['RF'], detected_bursts_RF, window, left_shift, 'RF')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a917103daca3829e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Defining Variables to store the total number of bursts for every muscle \n",
    "tot_bursts_TA = []; tot_bursts_MG = []; tot_bursts_SOL= []; tot_bursts_BF = []\n",
    "tot_bursts_ST = []; tot_bursts_VL = []; tot_bursts_RF = []\n",
    "burst_count_list_for_printing = []\n",
    "\n",
    "# Calculating the total number of bursts per muscle\n",
    "for subject_idx, subject in enumerate(subjects):\n",
    "    tot_bursts_TA.append(len(fixed_bursts_TA[subject_idx]))\n",
    "    tot_bursts_MG.append(len(fixed_bursts_MG[subject_idx]))\n",
    "    tot_bursts_SOL.append(len(fixed_bursts_SOL[subject_idx]))\n",
    "    tot_bursts_BF.append(len(fixed_bursts_BF[subject_idx]))\n",
    "    tot_bursts_ST.append(len(fixed_bursts_ST[subject_idx]))\n",
    "    tot_bursts_VL.append(len(fixed_bursts_VL[subject_idx]))\n",
    "    tot_bursts_RF.append(len(fixed_bursts_RF[subject_idx]))\n",
    "    # Saving the results in a list for fancy printing\n",
    "    burst_count_list_for_printing.append([subject, tot_bursts_TA[subject_idx], tot_bursts_MG[subject_idx], tot_bursts_SOL[subject_idx],\n",
    "                                          tot_bursts_BF[subject_idx], tot_bursts_ST[subject_idx], tot_bursts_VL[subject_idx],\n",
    "                                          tot_bursts_RF[subject_idx]])\n",
    "\n",
    "# Adding the total row to the printing\n",
    "burst_count_list_for_printing.append([])\n",
    "burst_count_list_for_printing.append(['---'])\n",
    "burst_count_list_for_printing.append(['Total Sum', sum(tot_bursts_TA), sum(tot_bursts_MG), sum(tot_bursts_SOL), sum(tot_bursts_BF), sum(tot_bursts_ST), sum(tot_bursts_VL), sum(tot_bursts_RF)])\n",
    "\n",
    "# Printing the table\n",
    "print(\"Number of Muscle Bursts Per Subject Per Muscle:\\n\")\n",
    "headers = [\"Subject\", \"TA Bursts\", \"MG Bursts\", \"SOL Bursts\", \"BF Bursts\", \"ST Bursts\", \"VL Bursts\", \"RF Bursts\"]\n",
    "print(tabulate(burst_count_list_for_printing, headers=headers))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93b757e534d75473",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Extracting the LOO 300-fixed bursts\n",
    "loo_fixed_bursts_TA = extract_burst_windows(loo_emg_signal_TA, loo_detected_bursts_TA, window, left_shift, 'Leave one Out: TA')[0]\n",
    "loo_fixed_bursts_MG = extract_burst_windows(loo_emg_signal_MG, loo_detected_bursts_MG, window, left_shift, 'Leave one Out: MG')[0]\n",
    "loo_fixed_bursts_SOL= extract_burst_windows(loo_emg_signal_SOL,loo_detected_bursts_SOL,window, left_shift, 'Leave one Out: SOL')[0]\n",
    "loo_fixed_bursts_BF = extract_burst_windows(loo_emg_signal_BF, loo_detected_bursts_BF, window, left_shift, 'Leave one Out: BF')[0]\n",
    "loo_fixed_bursts_ST = extract_burst_windows(loo_emg_signal_ST, loo_detected_bursts_ST, window, left_shift, 'Leave one Out: ST')[0]\n",
    "loo_fixed_bursts_VL = extract_burst_windows(loo_emg_signal_VL, loo_detected_bursts_VL, window, left_shift, 'Leave one Out: VL')[0]\n",
    "loo_fixed_bursts_RF = extract_burst_windows(loo_emg_signal_RF, loo_detected_bursts_RF, window, left_shift, 'Leave one Out: RF')[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "166b4611b9b9efe4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "loo_burst_count_list_for_printing = [[\"Leave-One-Out\", len(loo_fixed_bursts_TA), len(loo_fixed_bursts_MG), len(loo_fixed_bursts_SOL), len(loo_fixed_bursts_BF), len(loo_fixed_bursts_ST), len(loo_fixed_bursts_VL), len(loo_fixed_bursts_RF)]]\n",
    "\n",
    "# Printing the table\n",
    "print(\"Number of Muscle Bursts of the Leave-One-Out Subject:\\n\")\n",
    "headers = [\"Subject\", \"TA\", \"MG\", \"SOL\", \"BF\", \"ST\", \"VL\", \"RF\"]\n",
    "print(tabulate(loo_burst_count_list_for_printing, headers=headers))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b78c790944f5201",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving Leave One Out as TFRecord"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4939f3acf61b67fc"
  },
  {
   "cell_type": "code",
   "source": [
    "loo_emg_series_complete = [loo_fixed_bursts_TA, loo_fixed_bursts_MG, loo_fixed_bursts_SOL,\n",
    "                           loo_fixed_bursts_BF, loo_fixed_bursts_ST, loo_fixed_bursts_VL,\n",
    "                           loo_fixed_bursts_RF]\n",
    "\n",
    "muscle_groups = len(loo_emg_series_complete)\n",
    "\n",
    "with tf.io.TFRecordWriter('tfrecords/leave_one_out_700ms.tfrecord') as tfrecord:\n",
    "    for emg_muscle in tqdm(range(muscle_groups), desc=\"Extracting dataset to TFRecords\"):\n",
    "        for sample in loo_emg_series_complete[emg_muscle]:\n",
    "            # Prepare the features for TFRecord\n",
    "            features = {\n",
    "                'label': tf.train.Feature(float_list=tf.train.FloatList(value=tf.keras.utils.to_categorical(emg_muscle, 7))),\n",
    "                'feature': tf.train.Feature(float_list=tf.train.FloatList(value=sample))\n",
    "            }\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "            tfrecord.write(example.SerializeToString())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c1e0e6de091023c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Iterate over the whole dataset to count records/samples (https://www.rustyrobotics.com/posts/tensorflow/tfdataset-record-count/)\n",
    "# Reference: https://www.rustyrobotics.com/posts/tensorflow/tfdataset-record-count/\n",
    "def countRecords(ds:tf.data.Dataset):\n",
    "    count = 0\n",
    "    if tf.executing_eagerly():\n",
    "        # TF v2 or v1 in eager mode\n",
    "        for _ in ds:\n",
    "            count = count+1\n",
    "    else:\n",
    "        # TF v1 in non-eager mode\n",
    "        iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
    "        next_batch = iterator.get_next()\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            try:\n",
    "                while True:\n",
    "                    sess.run(next_batch)\n",
    "                    count = count+1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "    return count"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a00cee24446df4ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 1024"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3003362a8f402880",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def loo_read_tfrecord(serialized_example):\n",
    "    tfrecord_format = (\n",
    "        {\n",
    "            'label': tf.io.FixedLenFeature([7], tf.float32),  # MUSCLE LABEL: Adjusted for one-hot encoded labels\n",
    "            'feature': tf.io.FixedLenFeature([window], tf.float32),  # sEMG VALUE: Adjusted for features\n",
    "        }\n",
    "    )\n",
    "    example = tf.io.parse_single_example(serialized_example, tfrecord_format)\n",
    "    f = tf.reshape(example['feature'], [window, 1])  # Reshape if needed, here it's kept for consistency\n",
    "    f.set_shape([window, 1])\n",
    "    return f, example['label']\n",
    "\n",
    "def loo_get_dataset(tf_record_name):\n",
    "    dataset = tf.data.TFRecordDataset(tf_record_name)\n",
    "    dataset = dataset.map(loo_read_tfrecord, num_parallel_calls=AUTOTUNE)\n",
    "    dataset_samples = countRecords(dataset)\n",
    "    dataset = dataset.shuffle(dataset_samples)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "695a5f27fd6b6c6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "loo_dataset = loo_get_dataset('tfrecords/leave_one_out_700ms.tfrecord')",
   "metadata": {
    "collapsed": false
   },
   "id": "d5d7a30cfd03b99b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for feature, label in loo_dataset:\n",
    "    print(f'label={label.shape}, feature={feature.shape}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7aa4e1e96e5f42c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TFRecords: Storing Training and Validation Datasets in Tensorflow Records\n",
    "\n",
    "Reference: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/tfrecord.ipynb#scrollTo=_e3g9ExathXP"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2833a9f67e2ae0f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write TFRecords"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a420f2551effee29"
  },
  {
   "cell_type": "code",
   "source": [
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def extract_burst_windows_tfrecord(emg_series_complete, onset_lists, window_size, left_shift_value):\n",
    "    # Ensure unique file names for parallel processing or repeated calls\n",
    "    file_name = 'tfrecords/all_dataset.tfrecord'\n",
    "    with tf.io.TFRecordWriter(file_name) as tfrecord:\n",
    "        for muscle_index in tqdm(range(len(emg_series_complete)), desc=\"Extracting dataset to TFRecords (Fixed Window)\"):\n",
    "            for subject_index in range(len(onset_lists[muscle_index])):\n",
    "                burst_count = 0\n",
    "                ii = -1\n",
    "                for onset in onset_lists[muscle_index][subject_index][0]:\n",
    "                    ii += 1\n",
    "                    onset_ms = int(onset * 1000) - left_shift_value\n",
    "                    current_sample_window = []\n",
    "                    if onset != onset_lists[muscle_index][subject_index][0][-1]:\n",
    "                        if (onset_ms + window_size) < (onset_lists[muscle_index][subject_index][0][ii + 1] * 1000):\n",
    "                            burst_count = burst_count + 1\n",
    "                            for time_step in range(window_size):\n",
    "                                current_sample_window.append(emg_series_complete[muscle_index][onset_ms + time_step])\n",
    "                    else:\n",
    "                        burst_count = burst_count + 1\n",
    "                        for time_step in range(window_size):\n",
    "                            current_sample_window.append(emg_series_complete[muscle_index][onset_ms + time_step])\n",
    "\n",
    "                    if current_sample_window:\n",
    "                        current_sample_window -= np.mean(current_sample_window)\n",
    "                        # Convert your sample and label to appropriate tf.train.Feature formats\n",
    "                        features = {\n",
    "                            'label': _int64_feature(muscle_index),\n",
    "                            'feature': tf.train.Feature(float_list=tf.train.FloatList(value=current_sample_window)),\n",
    "                            'subject': _int64_feature(subject_index + 1),\n",
    "                            'burst': _int64_feature(burst_count)\n",
    "                        }\n",
    "                        example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "                        tfrecord.write(example.SerializeToString())\n",
    "    return file_name"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1ce5791af57c5dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# .....\n",
    "emg_series_tot = [df_all_subjects['TA'], df_all_subjects['MG'], df_all_subjects['SOL'],\n",
    "                  df_all_subjects['BF'], df_all_subjects['ST'], df_all_subjects['VL'],\n",
    "                  df_all_subjects['RF']]\n",
    "\n",
    "detected_bursts_tot = [detected_bursts_TA, detected_bursts_MG, detected_bursts_SOL,\n",
    "                       detected_bursts_BF, detected_bursts_ST, detected_bursts_VL,\n",
    "                       detected_bursts_RF]\n",
    "# Extracting bursts\n",
    "extract_burst_windows_tfrecord(emg_series_tot, detected_bursts_tot,  window, left_shift)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28d034070fed2670",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read TFRecords"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f069e88275cf16b6"
  },
  {
   "cell_type": "code",
   "source": [
    "def map_fn(serialized_example):\n",
    "    features = {\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'feature': tf.io.FixedLenFeature([window], tf.float32),\n",
    "        'subject': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'burst':  tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(serialized_example, features)\n",
    "    return example['label'], example['feature'], example['subject'], example['burst']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adbbf7e6af15aea1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = tf.data.TFRecordDataset('tfrecords/all_dataset.tfrecord')\n",
    "dataset = dataset.map(map_fn)\n",
    "\n",
    "for label, feature, subject, burst in dataset.take(10):\n",
    "    print(f'label={label}, Number of features={len(feature)}  subject={subject}, burst={burst}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db53937b1f6b0386",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def separate_dataset_per_subject_train_val(dataset, subj, train_percentage):\n",
    "    # Filtering whole dataset TFRECORDS by subjects:\n",
    "    dataset_subject = dataset.filter(lambda label,feature,subject,burst: subject==subj)\n",
    "    # Count Total Samples for each Subject Dataset\n",
    "    dataset_subject_samples = countRecords(dataset_subject)\n",
    "    # Shuffling bursts per subject\n",
    "    dataset_subject_shuffled = dataset_subject.shuffle(dataset_subject_samples)\n",
    "    # Separating Subject Training and Evaluation Datasets:\n",
    "    dataset_subject_1_train = dataset_subject_shuffled.take(int(dataset_subject_samples*train_percentage))\n",
    "    dataset_subject_1_val = dataset_subject_shuffled.skip(int(dataset_subject_samples*train_percentage)).take(dataset_subject_samples - int(dataset_subject_samples*train_percentage))\n",
    "    return dataset_subject_1_train, dataset_subject_1_val"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a60fc34550fd0511",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Training/Validation Split\n",
    "train_percentage = 0.8\n",
    "dataset_subject1_train, dataset_subject1_val = separate_dataset_per_subject_train_val(dataset, 1, train_percentage)\n",
    "dataset_subject2_train, dataset_subject2_val = separate_dataset_per_subject_train_val(dataset, 2, train_percentage)\n",
    "dataset_subject3_train, dataset_subject3_val = separate_dataset_per_subject_train_val(dataset, 3, train_percentage)\n",
    "dataset_subject4_train, dataset_subject4_val = separate_dataset_per_subject_train_val(dataset, 4, train_percentage)\n",
    "dataset_subject5_train, dataset_subject5_val = separate_dataset_per_subject_train_val(dataset, 5, train_percentage)\n",
    "dataset_subject6_train, dataset_subject6_val = separate_dataset_per_subject_train_val(dataset, 6, train_percentage)\n",
    "dataset_subject7_train, dataset_subject7_val = separate_dataset_per_subject_train_val(dataset, 7, train_percentage)\n",
    "dataset_subject8_train, dataset_subject8_val = separate_dataset_per_subject_train_val(dataset, 8, train_percentage)\n",
    "dataset_subject9_train, dataset_subject9_val = separate_dataset_per_subject_train_val(dataset, 9, train_percentage)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0375b2c1ecb8fe4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for label, feature, subject, burst in dataset_subject1_train.take(10):\n",
    "    print(f'label={label}, Number of features={len(feature)}  subject={subject}, burst={burst}')"
   ],
   "id": "c2482a04dd65b597"
  },
  {
   "cell_type": "code",
   "source": [
    "# ???????\n",
    "all_subject_datasets_train = [dataset_subject1_train, dataset_subject2_train, dataset_subject3_train,\n",
    "                              dataset_subject4_train, dataset_subject5_train, dataset_subject6_train,\n",
    "                              dataset_subject7_train, dataset_subject8_train, dataset_subject9_train]\n",
    "\n",
    "all_subject_datasets_val =   [dataset_subject1_val, dataset_subject2_val, dataset_subject3_val,\n",
    "                              dataset_subject4_val, dataset_subject5_val, dataset_subject6_val,\n",
    "                              dataset_subject7_val, dataset_subject8_val, dataset_subject9_val]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94a7cd59e9b8e05c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ????\n",
    "def augment_datasets(collection_datasets, tf_record_name):\n",
    "    with tf.io.TFRecordWriter(tf_record_name) as tfrecord:\n",
    "        for d in collection_datasets:\n",
    "            for label, feature, subject, burst in d:\n",
    "                features = {\n",
    "                    'label': tf.train.Feature(int64_list=tf.train.Int64List(value=np.asarray([label]))),\n",
    "                    'feature': tf.train.Feature(float_list=tf.train.FloatList(value=np.asarray(feature))),\n",
    "                    'subject': tf.train.Feature(int64_list=tf.train.Int64List(value=np.asarray([subject]))),\n",
    "                    'burst': tf.train.Feature(int64_list=tf.train.Int64List(value=np.asarray([burst])))\n",
    "                }\n",
    "                example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "                tfrecord.write(example.SerializeToString())\n",
    "    return\n",
    "\n",
    "def map_fn_final(serialized_example):\n",
    "    features = {\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'feature': tf.io.FixedLenFeature([window], tf.float32),\n",
    "        'subject': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'burst': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(serialized_example, features)\n",
    "    return example['label'], example['feature'], example['subject'], example['burst']\n",
    "\n",
    "def mix_shuffle_and_save_datasets(tf_record_read, tf_record_write):\n",
    "    dataset = tf.data.TFRecordDataset(tf_record_read)\n",
    "    dataset = dataset.map(map_fn_final)\n",
    "    dataset_samples = countRecords(dataset)\n",
    "    dataset_shuffled = dataset.shuffle(dataset_samples)\n",
    "\n",
    "    with tf.io.TFRecordWriter(tf_record_write) as tfrecord:\n",
    "        for label, feature, subject, burst in dataset_shuffled:\n",
    "            features = {\n",
    "                'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label.numpy()])),\n",
    "                'feature': tf.train.Feature(float_list=tf.train.FloatList(value=feature.numpy())),\n",
    "                'subject': tf.train.Feature(int64_list=tf.train.Int64List(value=[subject.numpy()])),\n",
    "                'burst': tf.train.Feature(int64_list=tf.train.Int64List(value=[burst.numpy()]))\n",
    "            }\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "            tfrecord.write(example.SerializeToString())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe00b17fb70892b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Call the functions in the correct order\n",
    "augment_datasets(all_subject_datasets_train, 'tfrecords/augmented_train_700ms.tfrecord')\n",
    "augment_datasets(all_subject_datasets_val, 'tfrecords/augmented_val_700ms.tfrecord')\n",
    "\n",
    "mix_shuffle_and_save_datasets('tfrecords/augmented_train_700ms.tfrecord', 'tfrecords/all_mixed_train_700ms.tfrecord')\n",
    "mix_shuffle_and_save_datasets('tfrecords/augmented_val_700ms.tfrecord', 'tfrecords/all_mixed_val_700ms.tfrecord')"
   ],
   "id": "398fa2070d857f41",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load TFRecords"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dac32b33347666fa"
  },
  {
   "cell_type": "code",
   "source": [
    "def read_tfrecord(serialized_example, export_subject=False):\n",
    "    tfrecord_format = (\n",
    "        {\n",
    "            'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'feature': tf.io.FixedLenFeature([window], tf.float32),\n",
    "            'subject': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'burst':  tf.io.FixedLenFeature([], tf.int64)\n",
    "        }\n",
    "    )\n",
    "    example = tf.io.parse_single_example(serialized_example, tfrecord_format)\n",
    "    f = tf.reshape(example['feature'], [window,1])\n",
    "    f.set_shape([window, 1])\n",
    "    # One-hot encode the label to match the expected shape for categorical_crossentropy\n",
    "    label = tf.one_hot(example['label'], depth=7)\n",
    "    if export_subject:\n",
    "        return f, label, example['subject']\n",
    "    return f, label\n",
    "\n",
    "def get_dataset(tf_record_name, train_or_valid):\n",
    "    # dataset = load_dataset(filename)\n",
    "    dataset = tf.data.TFRecordDataset(tf_record_name)\n",
    "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n",
    "    dataset_samples = countRecords(dataset)\n",
    "    print(f\"Number of {train_or_valid} Samples: \", dataset_samples)\n",
    "    dataset = dataset.shuffle(dataset_samples)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48547a8332d332b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = get_dataset('tfrecords/all_mixed_train_700ms.tfrecord', 'train')\n",
    "valid_dataset = get_dataset('tfrecords/all_mixed_val_700ms.tfrecord', 'validation')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e9f683dfc88431e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Verifying the shapes of: LOO, Training and Validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46dcef478b869dbf"
  },
  {
   "cell_type": "code",
   "source": [
    "for feature, label in loo_dataset:\n",
    "    print(f'Loo: label={label.shape}, feature={feature.shape}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a67f1632b2fc34a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for feature, label in train_dataset:\n",
    "    print(f'Train: label={label.shape}, feature={feature.shape}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d57b18567e25a41",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for feature, label in valid_dataset:\n",
    "    print(f'Validation: label={label.shape}, feature={feature.shape}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22217c55fb931b9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Plotting Muscle Bursts Samples",
   "metadata": {
    "collapsed": false
   },
   "id": "44b360a2c0a3a554"
  },
  {
   "cell_type": "code",
   "source": [
    "# # quick plot to see individual contraction bursts\n",
    "# def plot_independent_bursts(label, burst_list, color):\n",
    "#     n_cols = len(burst_list)  # number of subjects\n",
    "#     fig, axs = plt.subplots(1, n_cols, figsize=(15, 3), dpi=150)  # Adjusted figure size\n",
    "#     fig.subplots_adjust(top=0.85)  # Adjust top spacing\n",
    "#     fig.suptitle(f'Contraction Bursts: {label} (300ms Fixed Length)', y=0.95)  # Adjust title position\n",
    "# \n",
    "#     for j in range(n_cols):\n",
    "#         axs[j].plot(burst_list[j][0], color=color)  # Assuming you want the first burst\n",
    "#         axs[j].set_title(f'1st Burst of subject {j+1}', fontsize=10)\n",
    "#         axs[j].set_xlabel('time (ms)', fontsize=8)\n",
    "#         axs[j].set_ylabel('EMG', fontsize=8)\n",
    "#         axs[j].label_outer()  # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "# \n",
    "#     plt.tight_layout(pad=2.0, w_pad=0.5)  # Dynamically adjust layout\n",
    "#     plt.show()\n",
    "# \n",
    "# plot_independent_bursts(\"TA\", fixed_bursts_TA, \"cornflowerblue\")\n",
    "# plot_independent_bursts(\"MG\", fixed_bursts_MG, \"orange\")\n",
    "# plot_independent_bursts(\"SOL\",fixed_bursts_SOL,\"red\")\n",
    "# plot_independent_bursts(\"BF\", fixed_bursts_BF, \"green\")\n",
    "# plot_independent_bursts(\"ST\", fixed_bursts_ST, \"orange\")\n",
    "# plot_independent_bursts(\"VL\", fixed_bursts_VL, \"cyan\")\n",
    "# plot_independent_bursts(\"RF\", fixed_bursts_RF, \"brown\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dcb34cab3aec956",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hybrid CNN-LSTM Model Implementation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9faf25c975935eb2"
  },
  {
   "cell_type": "code",
   "source": [
    "# Parameters\n",
    "# epochs = 300 # For full performance test, seems okay \n",
    "epochs = 150 # For quick performance testing\n",
    "lr = 0.01 \n",
    "scheduler = ReduceLROnPlateau(factor=0.1, patience=15, min_lr=0.0001)  # Adjust patience as needed!!!!!!!\n",
    "\n",
    "    ## CNN Layer 1\n",
    "conv1D_1 = [32,5,1]  # number_filters,kernel_size and strides \n",
    "# conv1D_1 = [64,3,1]  # number_filters,kernel_size and strides \n",
    "dropout_1= 0.3      # Dropout %\n",
    "pool1D_1 = [2,2]     # pool_size and strides\n",
    "\n",
    "    ## CNN Layer 2\n",
    "conv1D_2 = [64,7,1]  # number_filters,kernel_size and strides\n",
    "# conv1D_2 = [96,5,1]  # number_filters,kernel_size and strides\n",
    "dropout_2= 0.3      # Dropout %\n",
    "pool1D_2 = [2,2]     # pool_size and strides\n",
    "\n",
    "#     ## CNN Layer 3\n",
    "conv1D_3 = [128,9,1] # number_filters,kernel_size and strides\n",
    "# conv1D_3 = [256,7,1] # number_filters,kernel_size and strides\n",
    "dropout_3= 0.3      # Dropout %\n",
    "pool1D_3 = [2,2]     # pool_size and strides\n",
    "\n",
    "    # Dense 1\n",
    "dense_1  = 50        # nodes ->50\n",
    "dense_dropout_1 = 0.3# Dropout %\n",
    "    # Dense 2\n",
    "dense_2 = 0          # nodes ->0\n",
    "    # LSTM 1             \n",
    "lstm_1   = 30        # lstm blocks ->30\n",
    "lstm_dropout_1 = 0.3 # Dropout %\n",
    "    # Dense 3\n",
    "dense_3  = 15        # nodes ->15\n",
    "dense_dropout_3 = 0.3# Dropout %\n",
    "\n",
    "# Definition\n",
    "model1 = Sequential()\n",
    "model1.add(InputLayer((window,1))) #InputLayer(BURST_WINDOW, N_CHANNELS)\n",
    "\n",
    "# CNN LAYER 1 (Conv1D + PReLU + MaxPooling + Dropout)\n",
    "model1.add(Conv1D(filters=conv1D_1[0],kernel_size=conv1D_1[1], strides=conv1D_1[2],padding='same', name='cnn_layer_1')) # TODO TRY WITH HIGHER KERNEL SIZE (ODD NUMBER!)\n",
    "model1.add(LeakyReLU(negative_slope=0.1))\n",
    "model1.add(MaxPooling1D(pool_size=pool1D_1[0], strides=pool1D_1[1], padding='same'))\n",
    "model1.add(Dropout(dropout_1))\n",
    "\n",
    "# CNN LAYER 2 (Conv1D + PReLU + MaxPooling + Dropout)\n",
    "model1.add(Conv1D(filters=conv1D_2[0], kernel_size=conv1D_2[1], strides=conv1D_2[2], padding='same', name='cnn_layer_2'))\n",
    "model1.add(LeakyReLU(negative_slope=0.1))\n",
    "model1.add(MaxPooling1D(pool_size=pool1D_2[0], strides=pool1D_2[1], padding='same'))\n",
    "model1.add(Dropout(dropout_2))\n",
    "\n",
    "# CNN LAYER 3 (Conv1D + LeakyReLU + MaxPooling + Dropout)\n",
    "model1.add(Conv1D(filters=conv1D_3[0], kernel_size=conv1D_3[1], strides=conv1D_3[2], padding='same', name='cnn_layer_3'))\n",
    "model1.add(LeakyReLU(negative_slope=0.1))\n",
    "model1.add(MaxPooling1D(pool_size=pool1D_3[0], strides=pool1D_3[1], padding='same'))\n",
    "model1.add(Dropout(dropout_3))\n",
    "\n",
    "# Global Average Pooling\n",
    "model1.add(GlobalAveragePooling1D())\n",
    "\n",
    "# Dense 1:  To integrate the Dense Layer 1 effectively after GAP, we reshape the output to make it compatible with the dense layer expectations\n",
    "model1.add(Reshape((1, -1))) \n",
    "model1.add(Dense(dense_1))\n",
    "model1.add(LeakyReLU(negative_slope=0.1))\n",
    "model1.add(Dropout(dense_dropout_1))\n",
    "\n",
    "# LSTM LAYER 1 + Dropout\n",
    "model1.add(LSTM(lstm_1, dropout=lstm_dropout_1))\n",
    "\n",
    "# Dense 3\n",
    "model1.add(Dense(dense_3)) \n",
    "model1.add(LeakyReLU(negative_slope=0.1))\n",
    "model1.add(Dropout(dense_dropout_3))\n",
    "\n",
    "# Softmax\n",
    "model1.add(Dense(7, 'softmax')) # Softmax\n",
    "\n",
    "#Summary\n",
    "model1.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e1ce08a7df3b743",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Starting the wandb logging & Parameters\n",
    "config = {\n",
    "    \"number of muscles\": len(muscles),\n",
    "    \"number of subjects\": len(list_of_all_subjects_dfs)+1,\n",
    "    \"leave-one-out SubjectID\": leave_one_out,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\":lr,\n",
    "    \"epochs\": epochs,\n",
    "    \"scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"GAP\": \"On\",\n",
    "    \n",
    "    \"CNN_1\": conv1D_1,\n",
    "    \"CNN_1_Activation\": \"LeakyReLU\", \n",
    "    \"CNN_1_Pool\": pool1D_1,\n",
    "    \"CNN_1_dropout\": dropout_1,\n",
    "    \n",
    "    \"CNN_2\": conv1D_2,\n",
    "    \"CNN_2_Activation\": \"LeakyReLU\", \n",
    "    \"CNN_2_Pool\": pool1D_2,\n",
    "    \"CNN_2_dropout\": dropout_2,\n",
    "    \n",
    "    \"CNN_3\": conv1D_3,\n",
    "    \"CNN_3_Activation\": \"LeakyReLU\",\n",
    "    \"CNN_3_Pool\": pool1D_3,\n",
    "    \"CNN_3_dropout\": dropout_3,\n",
    "    \n",
    "    \"dense_1\": dense_1,\n",
    "    \"dense_1_dropout\": dense_dropout_1,\n",
    "    \n",
    "    \"lstm_1\": lstm_1,\n",
    "    \"lstm_1_dropout\": lstm_dropout_1,\n",
    "    \n",
    "    \"dense_3\": dense_3,\n",
    "    \"dense_3_dropout\": dense_dropout_3}\n",
    "\n",
    "wandb.init(project='Thesis', entity='firass-koli', config=config)\n",
    "\n",
    "optimizer = Adam(learning_rate=wandb.config.learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95ab9b2a04c48592",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ????????????????????????????????????\n",
    "model1.compile(loss=categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])  # TODO: Try DK (Learning rate)\n",
    "\n",
    "cp = ModelCheckpoint('model1/best_model_epoch_{epoch:02d}_val_acc_{val_accuracy:.4f}.keras', save_best_only=True, monitor='val_accuracy', mode='max')"
   ],
   "id": "f5bb3c2daa1ad5ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clear_and_create_directory(directory):\n",
    "    \"\"\"Check if a directory exists and clear it, then recreate it.\"\"\"\n",
    "    if os.path.exists(directory):\n",
    "        # Remove the directory and all its contents\n",
    "        shutil.rmtree(directory)\n",
    "        print(f\"Old \\\"{directory}\\\" directory of the previous model got deleted!\")\n",
    "    # Create the directory again\n",
    "    os.makedirs(directory, exist_ok=True)"
   ],
   "id": "d3abfc9e9e0fcfcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Deleting the old directories of the previously trained models\n",
    "clear_and_create_directory(\"model1\")\n",
    "clear_and_create_directory(\"histories\")\n",
    "clear_and_create_directory(\"results\")"
   ],
   "id": "42fed72ce4d6e21d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# with tf.device('/gpu:0'):\n",
    "# Training the model with the wandb callback\n",
    "history = model1.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=wandb.config.epochs,\n",
    "    callbacks=[cp, scheduler, WandbMetricsLogger(log_freq=5)])\n",
    "             \n",
    "# Log the best validation accuracy and loss\n",
    "wandb.log({\"best_val_accuracy\": max(history.history['val_accuracy']), \"min_val_loss\": min(history.history['val_loss'])})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66459845c8a1d3d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# # In case of keyboard interrupt \n",
    "# # Log the best validation accuracy and loss manually\n",
    "# wandb.log({\"best_val_accuracy\": ???, \"min_val_loss\": ???})\n",
    "# wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd0d408e971e3f92",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Retrieve minimum loss and best accuracy\n",
    "min_val_categ_crossentropy = round(min(history.history['val_loss']),3)\n",
    "best_val_accuracy = round(max(history.history['val_accuracy']),3)*100\n",
    "# save_pickle(history, history_name+'.pkl')\n",
    "history_name = f'histories/history(val_acc={best_val_accuracy}%,val_categ_crossentropy={min_val_categ_crossentropy})'\n",
    "np.save(history_name + '.npy',history.history)\n",
    "# NOTE: The warnings you will see in the training are not relevant (it's due to the fact that the model is being saved so to be able to call it back in the future)\n",
    "print(f'History (loss and accuracy) for training and validation saved in:\\n-> {history_name}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43e7edc6aa51a84a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting Loss and Accuracy Metrics over Epochs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ff43229e70679ff"
  },
  {
   "cell_type": "code",
   "source": [
    "h = np.load(history_name+'.npy', allow_pickle=True).item()\n",
    "\n",
    "min_val_categ_crossentropy = round(min(h['val_loss']), 4)\n",
    "best_val_accuracy = round(max(h['val_accuracy']), 3) * 100\n",
    "\n",
    "# Find the epoch with the best validation accuracy\n",
    "best_val_acc_epoch = np.argmax(h['val_accuracy'])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle('Hybrid CNN + LSTM: Loss and Accuracy over Epochs', fontsize=18, y=0.99)\n",
    "ax1.set_title(f'Loss Function (min_categ_crossentropy={min_val_categ_crossentropy})')\n",
    "ax2.set_title(f'Accuracy (best_val_acc={best_val_accuracy}%)')\n",
    "ax1.set(xlabel='Epoch', ylabel='Loss (Categorical Crossentropy)')\n",
    "ax2.set(xlabel='Epoch', ylabel='Accuracy')\n",
    "ax1.plot(h['loss'], color=\"cornflowerblue\", linewidth=3)\n",
    "ax1.plot(h['val_loss'], color=\"lightsteelblue\", linewidth=3)\n",
    "ax1.legend(['Training Loss', 'Validation Loss'])\n",
    "ax2.plot(h['accuracy'], color=\"gold\", linewidth=3)\n",
    "ax2.plot(h['val_accuracy'], color=\"darkorange\", linewidth=3)\n",
    "ax2.legend(['Training Accuracy', 'Validation Accuracy'])\n",
    "\n",
    "# Add a red 'X' mark at the epoch where the best validation accuracy occurs\n",
    "ax2.scatter(best_val_acc_epoch, h['val_accuracy'][best_val_acc_epoch], color='red', marker='X', s=100)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Saving the figure with the best validation accuracy mark\n",
    "plt.savefig(history_name + '.png')\n",
    "\n",
    "model_name = f'results/val_acc={best_val_accuracy}%,val_categ_crossentropy={min_val_categ_crossentropy}'\n",
    "plt.savefig(model_name + '.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d20dfe6f143753e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Serializing Datasets (train and val)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b428b69350c0f16"
  },
  {
   "cell_type": "code",
   "source": [
    "X_train = [] ; y_train = []\n",
    "X_val = [] ; y_val = []\n",
    "X_loo = [] ; y_loo = []\n",
    "\n",
    "for feature, label in train_dataset:\n",
    "    for i in range(label.shape[0]):\n",
    "        X_train.append(feature[i])\n",
    "        y_train.append(label[i])\n",
    "for feature, label in valid_dataset:\n",
    "    for i in range(label.shape[0]):\n",
    "        X_val.append(feature[i])\n",
    "        y_val.append(label[i])\n",
    "for feature, label in loo_dataset:\n",
    "    for i in range(label.shape[0]):\n",
    "        X_loo.append(feature[i])\n",
    "        y_loo.append(label[i])\n",
    "\n",
    "X_train = np.array(X_train) ; y_train = np.array(y_train)\n",
    "X_val = np.array(X_val) ; y_val = np.array(y_val)\n",
    "X_loo = np.array(X_loo) ; y_loo = np.array(y_loo)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa524e5d6a68f2b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metric Functions: Confusion Matrix, Precision, Recall and F-1Score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97c02338771e288f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Find the filename of the best model saved during training\n",
    "best_model_filename = max(glob.glob('model1/best_model_epoch_*_val_acc_*.keras'), key=os.path.getctime)\n",
    "print(f\"Best Model: {best_model_filename}\")\n",
    "# Load the best model\n",
    "best_model = load_model(best_model_filename)"
   ],
   "id": "20b3bc628cf226c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_confusion_matrix(x,y,color,title):\n",
    "    predictions_hot = best_model.predict(x)\n",
    "    predictions = np.argmax(predictions_hot, axis=1)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    print(\"Some y Predicted\\t\", predictions[:30])\n",
    "    y_list = np.argmax(y, axis=1)\n",
    "    print(\"Some y Label\\t\\t\", y_list[:30])\n",
    "\n",
    "    cm = confusion_matrix(y_list, predictions)\n",
    "    plt.figure(figsize = (15,10))\n",
    "    ax = sn.heatmap(cm, annot=True, cmap=color, fmt='d')\n",
    "    ax.set_title(title+'\\n\\n')\n",
    "    ax.set_xlabel('Predicted Values')\n",
    "    ax.set_ylabel('Actual Values ')\n",
    "    ax.xaxis.set_ticklabels(['TA', 'MG', 'SOL', 'BF', 'ST', 'VL', 'RF'])\n",
    "    ax.yaxis.set_ticklabels(['TA', 'MG', 'SOL', 'BF', 'ST', 'VL', 'RF'])\n",
    "    \n",
    "    np.set_printoptions(precision=3)\n",
    "    precision, recall, f1, _ = score(np.argmax(y, axis=1), np.argmax(predictions_hot, axis=1))\n",
    "    f1_micro = f1_score(np.argmax(y, axis=1), np.argmax(predictions_hot, axis=1), average='micro')\n",
    "\n",
    "    print(f'precision: {precision}')\n",
    "    print(f'recall: {recall}')\n",
    "    print(f'fscore: {f1}')\n",
    "    print(f'fscore_micro: {f1_micro:.3f}')\n",
    "\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    plt.show()\n",
    "    \n",
    "    return precision, recall, f1, f1_micro"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1967f470cb94cf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confusion Matrix in Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a43b1033d978146"
  },
  {
   "cell_type": "code",
   "source": [
    "plot_confusion_matrix(X_train, y_train, 'Greens', 'Confusion Matrix Training')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5fbd75c0fbf2605",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confusion Matrix in Testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "770ec2764a474aa3"
  },
  {
   "cell_type": "code",
   "source": "testing_precision, testing_recall, testing_f1, testing_f1_micro  = plot_confusion_matrix(X_val, y_val, 'Blues', 'Confusion Matrix Validation Intrapersonal')",
   "metadata": {
    "collapsed": false
   },
   "id": "a26b64bc4e568260",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "interpersonal_precision, interpersonal_recall, interpersonal_f1, interpersonal_f1_micro = plot_confusion_matrix(X_loo, y_loo, 'Reds', 'Confusion Matrix Validation Interpersonal (Leave-One-Out)')",
   "metadata": {
    "collapsed": false
   },
   "id": "e69e1e96922f3571",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Log the best validation accuracy and loss\n",
    "wandb.log({\"testing_f1\": testing_f1, \"testing_f1_micro\": testing_f1_micro,\n",
    "           \"testing_precision\": testing_precision, \"testing_recall\": testing_recall,\n",
    "           \"interpersonal_f1\": interpersonal_f1, \"interpersonal_f1_micro\": interpersonal_f1_micro,\n",
    "           \"interpersonal_precision\": interpersonal_precision, \"interpersonal_recall\": interpersonal_recall})\n",
    "\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1634a8ccbe6398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d82d024a0456164b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a475ae5dae595bef",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
